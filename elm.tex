\section{Máquinas Extremas}\label{elmorig}
As máquinas extremas (\textit{Extreme Learning Machines} - ELMs) são redes - não
necessariamente neurais - com treinamento diferenciado que têm recebido crescente
atenção devido à sua simplicidade e performance, comparável com o estado da arte
\citep{journals/tsmc/HuangZDZ12}.
Trata-se de um algoritmo adequado para aprendizado ativo porque
seu ajuste de parâmetros pode ser simplificado de uma forma que não ocorre
com outros algoritmos similares em complexidade
(com a capacidade de aproximação universal)
e nichos de aplicação como a máquina
de vetores de suporte e o Perceptron multicamadas \citep{haykin2004comprehensive}.
Esse último requer, por exemplo, algum critério de parada no treinamento e um processo
de seleção de modelo que envolve o ajuste de topologia e parâmetros.
Essa característica da ELM permite um rápido treinamento,
algo necessário num ambiente interativo.
O motivo para a baixa necessidade ou ausência de iterações de ajuste é explicado a seguir.
% \citep{journals/tsmc/HuangZDZ12} 

Fixado o tipo de nó como o neurônio artificial convencional, seus parâmetros a ajustar são os
valores e quantidade de neurônios/conexões sinápticas da camada oculta.
Dada uma função geradora de pesos aleatórios ou pseudo-aleatórios incluída sua semente geradora e
adotada a restrição de sempre se manter os pesos na ordem em que são
gerados\footnote{Do contrário, cada peso deveria ser interpretado como um parâmetro independente.},
a quantidade de
neurônios $L$ na camada oculta pode ser vista como o único parâmetro a se ajustar.
Embora possa ser otimizado, esse parâmetro frequentemente não é crítico
\citep{conf/esann/FrenayV10}.
Essa possível não-criticidade torna a ELM, dentro das restrições apresentadas,
capaz de aprender num passo único e rápido quando comparada a outros algoritmos similares
\citep{journals/tsmc/HuangZDZ12}.

% descrever ELM básica
Uma ELM é uma rede com uma única camada oculta que transmite os sinais apenas na direção
entrada-saída (\textit{Single Hidden Layer Feedforward Network} - SLFN),
cujos nós podem ser neurais (aditivo logístico sigmoidal) - como neste trabalho -
ou de diversos outros tipos, como as também frequentemente usadas funções de base radial
\citep{journals/tsmc/HuangZDZ12}.

\ano{relembrar leitor da notação?}

A função preditiva de uma SLFN com $L$ neurônios ocultos pode ser representada como segue:
\begin{equation} \label{eqelm}
f(\bm{x}_{(t)})= \argmax_{\bm{y} \in Y}{\displaystyle\mathop{\sum} _{l=1}^{L}\beta_{l,c(\bm{y})}
g(\bm{a}_l \bm{x}_{(t)} + b_l)} = \bm{y}_{(t)}
\end{equation}
% }, \quad j=1, \cdots, {\it N}.

onde $\beta_{l,o}$ é o peso da sinapse que conecta o neurônio oculto $l$ ao neurônio de saída $o$;
$c(\bm{z})$ é uma função auxiliar que retorna o índice correspondente à classe
representado pelo vetor $\bm{z} \in Y$;
$g$ é, neste trabalho, a função sigmoide logística;
$\bm{a}_l$ é o vetor de pesos do neurônio $l$ com cada valor $a_i$ representando o peso entre a
entrada $i$ e o neurônio $l$ e, sendo o produto interno deste neurônio uma equação de reta no
espaço de parâmetros, $b_l$ é o valor de seu viés de deslocamento em relação à origem.
Mais detalhes da formulação da primeira camada oculta podem ser consultados na literatura sobre o
Perceptron multicamadas convencional \citep{haykin2004comprehensive}.
Se $\beta$ for adotada como uma matriz, a equação \ref{eqelm} pode ser escrita compactamente da
forma:
\begin{equation} \label{eq2}
H \beta = T
\end{equation}

onde $H$ é a \ing{matriz de saídas da camada oculta}{hidden layer output matrix} da SLFN,
ou seja, cada coluna corresponde à saída de um neurônio oculto e cada linha corresponde a um
exemplo do conjunto de treinamento;
$\beta$ é a matriz contendo os pesos que conectam a camada oculta à camada de saída, ou seja, cada
coluna corresponde a um neurônio de saída e cada linha corresponde a um neurônio oculto;
e $T$ é a matriz objetivo, que contém em cada linha a
representação binária $\bm{y}$ da classe de cada exemplo.

Calcular $\beta$, que é análogo a treinar a rede,
pode ser feito por meio do cálculo da pseudo-inversa $H^{\dagger}$ de $H$
\citep{rao1971generalized}:
\begin{equation} \label{eq3}
\beta = H^{\dagger} T
\end{equation}

\tar{concluir mais gentilmente}

\ano{ELM ajuda a reduzir overfitting com pequenas normas dos pesos.}

\tar {necessario apresentar versao multiclasse do press que precisei desenvolver}

referencias para press \citep{myers2000classical};
\citep{journals/ijon/HeeswijkMOL11}
refaz ELM (jeito antigo, sem lumda)  a cada grow e chama de eficiente.
posso usar esse como ponto de partida, mas não por na GPU porque:
1-AL tem pequenos cjtos de teste(rotulos); 2-overhead entre grows é
ainda maior por ser um crescimento ``incremental''.
o calculo do HAT vem de graça da EM-ELM que eles esqueceram de adotar.


\subsection{OS-ELM}\label{oselm}
Em sistemas interativos, frequentemente é adotado o esquema incremental de aprendizado.
O tempo de treinamento é reduzido quando é possível aproveitar o resultado de cálculos
prévios durante todo o processo.
No caso da ELM, isso é possível por meio da
\elm \textit{sequencial}
(\textit{On-line Sequential Extreme Learning Machine} - OS-ELM)
proposta por \cite{conf/iastedCI/HuangLRSS05}.
% cite Sherman-Morrison-Woodbury)
% http://books.google.com.br/books?hl=en&lr=&id=iD5s0iKXHP8C&oi=fnd&pg=PT15&dq=+An+introduction+to+optimization&ots=3PqthZAq8d&sig=Befcr8te239chWT6UOVyVrMIkSo#v=snippet&q=recursive%20least&f=false


A técnica é baseada no algoritmo dos mínimos quadrados recursivo
que utiliza a fórmula de Sherman-Morrison-Woodbury
\citep{chong2013introduction}.
$H$ passa a ser considerada em função do tempo - indicado subscrito
para melhor legibilidade: $H_{(t)}$.
Para ser possível encontrar a solução dos mínimos quadrados de
$H_{(0)} \beta_{(0)} = T_{(0)}$,
que se refere ao conjunto inicial de dados rotulados (instante $t=0$),
a pseudo-inversa esquerda $H^\dagger_{(0)}$ deve ser calculada por:

\begin{equation} \label{eq4}
P_{(0)}=(H^\top_{(0)}H_{(0)})^{-1}
\end{equation}
\begin{equation} \label{eq5}
H^{\dagger}_{(0)} = P_{(0)}H^\top_{(0)}
\end{equation}

Assim, é possível realizar um treinamento inicial da rede.
Para os lotes de dados subsequentes, ou seja,
quando $t>0$, apenas $\beta_{(t)}$ e $P_{(t)}$ precisam ser mantidos, onde:

\begin{equation} \label{eq6}
P_{(t)} = P_{(t-1)} - P_{(t-1)} H^{\top}_{(t)} (I + H_{(t)} P_{(t-1)} H^{\top}_{(t)})^{-1} H_{(t)}
P_{(t-1)}
\end{equation}
\begin{equation} \label{eq7}
\beta_{(t)} = \beta_{(t-1)} + P_{(t)} H^{\top}_{(t)} (T_{(t)} - H_{(t)} \beta_{(t-1)})
\end{equation}

O cálculo incremental de $\beta_{(t)}$ e $P_{(t)}$ evita os custos de se recalcular a
pseudo-inversa para cada novo lote de dados.

% Além da OS-ELM, outras variantes da ELM têm sido propostas:
% ROS-ELM, que evita matrizes mal-condicionadas por meio do ajuste dos vieses $b_i$ \cite
% {conf/isnn/HoangHVW07};
% EI-ELM ou EM-ELM, não-incrementais, que fazem a rede crescer \textit{congelando}
% \citep{huang2008enhanced} ou \textit{atualizando} \citep{journals/tnn/FengHLG09}
% nós antigos;
% e, CEOS-ELM, que faz a rede crescer durante o aprendizado \textit{on-line} \citep{conf/ijcnn/LanSH09}.
% 
% Para o presente trabalho, a OS-ELM ``canônica'' foi adotada, pois cada variante tem seus próprias
% vicissitudes e o propósito deste trabalho é apenas dar uma primeira impressão a respeito da
% performance da ELM com diferentes estratégias.
% Também é importante lembrar que, na presença de uma matriz $H_{(0)}$ mal-condicionada,
% OS-ELM não converge para ELM.

\ano{PRESS: erro estatistico (de regressão?)}

\ano{quadrado do resíduo: erro I e CI?}

diferencia 'statistical error' e 'residual':

{http://en.wikipedia.org/wiki/Errors\_and\_residuals\_in\_statistics}

error (or disturbance) of an observed value is the deviation of the observed value from
the (unobservable) true function value
residual of an observed value is the difference between the observed value and the
estimated function value.

\tar{Citar von Zuben e outros brasileiros}

\subsection{I-ELM}\label{ielm}
Apesar da vantagem do cálculo da pseudo-inversa ser uma etapa única no
aprendizado da ELM, trata-se de um cálculo que pode ser custoso.
Uma alternativa é a \elm \textit{incremental} (I-ELM) proposta por
\cite{journals/tnn/HuangCS06}.
O aprendizado da I-ELM começa com um neurônio e
progride com a adição de novos neurônios, um a um.
O valor do peso é calculado de forma a reduzir o erro entre o
valor predito e o valor esperado.
Cada neurônio adicionado na iteração $t$ requer um novo peso $\beta_{1,t}$
para conectá-lo ao neurônio de saída.
Assumindo, por simplicidade, apenas um atributo preditivo,
a matriz alvo $T$ se torna um \textit{vetor coluna}, representado aqui por $\bm{\tau}$.
Sendo $\bm{h}_{(t)}$ o vetor de valores da função de ativação do novo neurônio,
ou seja, com os valores da coluna $t$ de $H$,
o valor do peso é calculado segundo a Equação \ref{eq:ielm}.
\begin{equation}\label{eq:ielm0}
\bm{e}_{(0)} = \bm{\tau}
\end{equation}
\begin{equation}\label{eq:ielm}
\beta_{1,t}=\frac{\bm{e}_{(t-1)}\cdot \bm{h}_{(t)}}
{\bm{h}_{(t)}\cdot \bm{h}_{(t)}}
\end{equation}
Onde $\bm{e}_{(t)}$ é o vetor de erros residuais antes da inserção do
novo neurônio na iteração $t$.
Todos os vetores têm cada valor associado a um dos exemplos do conjunto de treinamento.

\subsection{CI-ELM}
Uma proposta similar à I-ELM, chamada \textit{convexa incremental} (CI-ELM),
opta pelo reajuste de todos os pesos da camada de saída a cada novo neurônio
acrescentado \citep{journals/ijon/HuangC07}.
Diferentemente da I-ELM, ela é baseada na otimização convexa de Barron
\citep{journals/tit/Barron93}, mas mantém a capacidade de redução do erro
treinamento de forma monotônica até qualquer valor arbitrariamente pequeno.
Ela é capaz de atingir uma convergência maior que a I-ELM
para um mesmo número de iterações.
Assim, dada uma meta de erro, a CI-ELM possibilita a indução de redes mais compactas.

Para cada novo neurônio, seu peso é calculado de forma análoga à apresentada na
Seção \ref{ielm}, porém os valores de saída do novo neurônio representados por
$\bm{h}_{(t)}$ são substituídos pela diferença $\bm{d}_{(t)}$
entre o erro corrente e o erro devido ao novo neurônio,
conforme equações \ref{eq:cielm} e \ref{eq:cielm2}.
\begin{equation}\label{eq:cielm}
\bm{d}_{(t)}=\bm{e}_{(t-1)} - (\bm{\tau} - \bm{h}_{(t)})
\end{equation}
\begin{equation}\label{eq:cielm2}
\beta_{1,t}=\frac{\bm{e}_{(t-1)}\cdot \bm{d}_{(t)}}
{\bm{d}_{(t)}\cdot \bm{d}_{(t)}}
\end{equation}
% Onde $T$ é o vetor alvo, ou seja, contém os valores do atributo preditivo.
A etapa adicional de reajuste é feita para todo o conjunto de pesos anteriormente
acrescentados ($\{\beta_i \mid 1 \leq i < t\}$)
de acordo com a Equação \ref{reajuste}.
\begin{equation}\label{reajuste}
\beta_{1,i}^* = (1 - \beta_{1,t}) \cdot \beta_{1,i}
\end{equation}
