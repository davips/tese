\section{Máquinas Extremas}\label{elmorig}
As máquinas extremas (\textit{Extreme Learning Machines} - ELMs) são redes - não
necessariamente neurais - com treinamento diferenciado que têm recebido crescente
atenção devido à sua simplicidade e performance, comparável com o estado da arte
\citep{journals/tsmc/HuangZDZ12}.
Trata-se de um algoritmo adequado para AA porque, ainda que pertença a um grupo
de algoritmos complexos, com a capacidade de aproximação universal
\citep{journals/tsmc/HuangZDZ12} - por exemplo,
ela tem um ajuste de parâmetros que pode ser simplificado de uma forma que não ocorre
com outros algoritmos similares em complexidade e nichos de aplicação como a máquina
de vetores de suporte e o Perceptron multicamadas \citep{haykin2004comprehensive}.
Esse último requer, por exemplo, algum critério de parada no treinamento e um processo
de seleção de modelo que envolve o ajuste de topologia e parâmetros.
Essa característica da ELM permite um rápido treinamento, algo necessário num ambiente interativo.
O motivo para a baixa necessidade ou ausência de iterações é explicado a seguir.

Fixado o tipo de nó como o neurônio artificial convencional, seus parâmetros a ajustar são os
valores e quantidade de neurônios/conexões sinápticas da camada oculta.
Dada uma função geradora de pesos aleatórios ou pseudo-aleatórios incluída sua semente geradora e
adotada a restrição de sempre se manter os pesos na ordem em que são
gerados\footnote{Do contrário, cada peso deveria ser interpretado como um parâmetro independente.},
a quantidade de
neurônios $L$ na camada oculta pode ser vista como o único parâmetro a se ajustar.
Embora possa ser otimizado, esse parâmetro frequentemente não é crítico
\citep{conf/esann/FrenayV10}.
Essa possível não-criticidade torna a ELM, dentro das restrições apresentadas,
capaz de aprender num passo único e rápido quando comparada a outros algoritmos similares
\citep{journals/tsmc/HuangZDZ12}.

% descrever ELM básica
Uma ELM é uma rede com uma única camada oculta que transmite os sinais apenas na direção
entrada-saída (\textit{Single Hidden Layer Feedforward Network} - SLFN),
cujos nós podem ser neurais (aditivo logístico sigmoidal) - como neste trabalho -
ou de diversos outros tipos, como as também frequentemente usadas funções de base radial
\citep{journals/tsmc/HuangZDZ12}.

Dados $Y$ o conjunto de vetores que representam todas as classes possíveis;
$\bm{x}_{(t)}$, um vetor de atributos descritivos pertencentes a um exemplo cuja classe se deseja
descobrir no instante $t \in \mathbb{N}$;
e $\bm{y}_{(t)} \in Y$, seu vetor associado de atributos preditivos que é uma representação
binária em que os valores respeitam a restrição de que para uma classe com
índice correspondente $o$,
$y_o=1$ e $y_p=0 \forall o \neq p, 1 \leq p \leq |Y|$;
a função preditiva de uma SLFN com $L$ neurônios ocultos pode ser representada como segue:
\begin{equation} \label{eqelm}
f(\bm{x}_{(t)})= \argmax_{\bm{y} \in Y}{\displaystyle\mathop{\sum} _{l=1}^{L}\beta_{l,c(\bm{y})}
g(\bm{a}_l \bm{x}_{(t)} + b_l)} = \bm{y}_{(t)}
\end{equation}
% }, \quad j=1, \cdots, {\it N}.

onde $\beta_{l,o}$ é o peso da sinapse que conecta o neurônio oculto $l$ ao neurônio de saída $o$;
$c(\bm{z})$ é uma função auxiliar que retorna o índice correspondente à classe
representado pelo vetor $\bm{z} \in Y$;
$g$ é, neste trabalho, a função sigmoide logística;
$\bm{a}_l$ é o vetor de pesos do neurônio $l$ com cada valor $a_i$ representando o peso entre a
entrada $i$ e o neurônio $l$ e, sendo o produto interno deste neurônio uma equação de reta no
espaço de parâmetros, $b_l$ é o valor de seu viés de deslocamento em relação à origem.
Mais detalhes da formulação da primeira camada oculta podem ser consultados na literatura sobre o
Perceptron multicamadas convencional \citep{haykin2004comprehensive}.
Se $\beta$ for adotada como uma matriz, a equação \ref{eqelm} pode ser escrita compactamente da
forma:
\begin{equation} \label{eq2}
H \beta = T
\end{equation}

onde $H$ é a matriz de saídas da camada oculta (\textit{hidden layer output matrix}) da SLFN,
ou seja, cada coluna corresponde à saída de um neurônio oculto e cada linha corresponde a um
exemplo do conjunto de treinamento;
$\beta$ é a matriz contendo os pesos que conectam a camada oculta à camada de saída, ou seja, cada
coluna corresponde a um neurônio de saída e cada linha corresponde a um neurônio oculto;
e $T$ é a matriz objetivo, que contém em cada linha a
representação binária $\bm{y}$ da classe de cada exemplo.

Calcular $\beta$, que é análogo a treinar a rede,
pode ser feito por meio do cálculo da pseudo-inversa $H^{\dagger}$ de $H$
\citep{rao1971generalized}:
\begin{equation} \label{eq3}
\beta = H^{\dagger} T
\end{equation}

\tar{concluir mais gentilmente}

\ano{ELM ajuda a reduzir overfitting com pequenas normas dos pesos.}

\tar {necessario apresentar versao multiclasse do press que precisei desenvolver}

referencias para press \citep{myers2000classical};
\citep{journals/ijon/HeeswijkMOL11}
refaz ELM (jeito antigo, sem lumda)  a cada grow e chama de eficiente.
posso usar esse como ponto de partida, mas não por na GPU porque:
1-AL tem pequenos cjtos de teste(rotulos); 2-overhead entre grows é
ainda maior por ser um crescimento ``incremental''.
o calculo do HAT vem de graça da EM-ELM que eles esqueceram de adotar.


\subsection{OS-ELM}\label{oselm}
Em sistemas interativos, frequentemente é adotado o esquema incremental de aprendizado.
O tempo de treinamento é reduzido quando é possível aproveitar o resultado de cálculos
prévios durante todo o processo.
No caso da ELM, isso é possível por meio da
\elm \textit{sequencial}
(\textit{On-line Sequential Extreme Learning Machine} - OS-ELM)
proposta por \cite{conf/iastedCI/HuangLRSS05}.
% cite Sherman-Morrison-Woodbury)
% http://books.google.com.br/books?hl=en&lr=&id=iD5s0iKXHP8C&oi=fnd&pg=PT15&dq=+An+introduction+to+optimization&ots=3PqthZAq8d&sig=Befcr8te239chWT6UOVyVrMIkSo#v=snippet&q=recursive%20least&f=false


A técnica é baseada no algoritmo dos mínimos quadrados recursivo
que utiliza a fórmula de Sherman-Morrison-Woodbury
\citep{chong2013introduction}.
$H$ passa a ser considerada em função do tempo - indicado subscrito
para melhor legibilidade: $H_{(t)}$.
Para ser possível encontrar a solução dos mínimos quadrados de
$H_{(0)} \beta_{(0)} = T_{(0)}$,
que se refere ao conjunto inicial de dados rotulados (instante $t=0$),
a pseudo-inversa esquerda $H^\dagger_{(0)}$ deve ser calculada por:

\begin{equation} \label{eq4}
P_{(0)}=(H^\top_{(0)}H_{(0)})^{-1}
\end{equation}
\begin{equation} \label{eq5}
H^{\dagger}_{(0)} = P_{(0)}H^\top_{(0)}
\end{equation}

Assim, é possível realizar um treinamento inicial da rede.
Para os lotes de dados subsequentes, ou seja,
quando $t>0$, apenas $\beta_{(t)}$ e $P_{(t)}$ precisam ser mantidos, onde:

\begin{equation} \label{eq6}
P_{(t)} = P_{(t-1)} - P_{(t-1)} H^{\top}_{(t)} (I + H_{(t)} P_{(t-1)} H^{\top}_{(t)})^{-1} H_{(t)}
P_{(t-1)}
\end{equation}
\begin{equation} \label{eq7}
\beta_{(t)} = \beta_{(t-1)} + P_{(t)} H^{\top}_{(t)} (T_{(t)} - H_{(t)} \beta_{(t-1)})
\end{equation}

O cálculo incremental de $\beta_{(t)}$ e $P_{(t)}$ evita os custos de se recalcular a
pseudo-inversa para cada novo lote de dados.

% Além da OS-ELM, outras variantes da ELM têm sido propostas:
% ROS-ELM, que evita matrizes mal-condicionadas por meio do ajuste dos vieses $b_i$ \cite
% {conf/isnn/HoangHVW07};
% EI-ELM ou EM-ELM, não-incrementais, que fazem a rede crescer \textit{congelando}
% \citep{huang2008enhanced} ou \textit{atualizando} \citep{journals/tnn/FengHLG09}
% nós antigos;
% e, CEOS-ELM, que faz a rede crescer durante o aprendizado \textit{on-line} \citep{conf/ijcnn/LanSH09}.
% 
% Para o presente trabalho, a OS-ELM ``canônica'' foi adotada, pois cada variante tem seus próprias
% vicissitudes e o propósito deste trabalho é apenas dar uma primeira impressão a respeito da
% performance da ELM com diferentes estratégias.
% Também é importante lembrar que, na presença de uma matriz $H_{(0)}$ mal-condicionada,
% OS-ELM não converge para ELM.

\ano{PRESS: erro estatistico (de regressão?)}

\ano{quadrado do resíduo: erro I e CI?}

diferencia 'statistical error' e 'residual':

{http://en.wikipedia.org/wiki/Errors\_and\_residuals\_in\_statistics}

error (or disturbance) of an observed value is the deviation of the observed value from
the (unobservable) true function value
residual of an observed value is the difference between the observed value and the
estimated function value.

\tar{Citar von Zuben e outros brasileiros}

\subsection{I-ELM}
Apesar da vantagem do cálculo da pseudo-inversa ser uma etapa única no
aprendizado da ELM, trata-se de um cálculo que pode ser custoso.
Uma alternativa é a \elm \textit{incremental} (I-ELM) proposta por
\cite{journals/tnn/HuangCS06}.
O aprendizado da I-ELM começa com um neurônio e
progride com a adição de novos neurônios, um a um.
Para cada neurônio adicionado, um novo peso é necessário para conectá-lo
ao neurônio de saída conforme a Equação \ref{eq:ielm}.
\begin{equation}\label{eq:ielm}
\beta_{n}={E\cdot H^{T}\over H\cdot H^{T}}
={\sum_{p=1}^{N}e(p)h(p)\over\sum_{p=1}^{N}h^{2}(p)}
\end{equation}
O valor do peso é calculado de forma a reduzir o erro entre o valor predito e o valor
esperado.
Onde $E$ é o vetor de erros residuais antes da inserção do
novo neurônio e $H$ é o vetor de valores da função de ativação do novo neurônio.
Ambos os vetores têm cada valor associado a um dos exemplos do conjunto de treinamento.

\subsection{CI-ELM}
\citep{journals/ijon/HuangC07}

