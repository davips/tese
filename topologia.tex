\subsection{Topologia}
A definição da topologia de uma rede neural é um problema em aberto.
\cite{sheela2013review}, por exemplo, citam cento e uma heurísticas para a definição do
número de neurônios ocultos.
\ano{TODO: apresentar ou não todas as elms que estão escondidas no tex?}

% On Over-?tting in Model Selection and Subsequent Selection Bias
% n Performance Evaluation (2010)
% % http://jmlr.org/papers/volume11/cawley10a/cawley10a.pdf
% is likely to be most severe when the sample of data is small and the number of
% hyper-parameters to be tuned is relatively large.
% namely regularisation (Cawley and Talbot, 2007), early stopping (Qi et al., 2004) and
% model or hyper-parameter averaging (Cawley, 2006; Hall and Robinson, 2009).%
% Another approach is simply to avoid model selection altogether using an ensemble%
% Kulkarni et al. (1998): relatively little is known about LOO properties.
% Bengio and Grandvalet (2004) have shown there is no unbiased estimate of the variance
% of (k-fold) cross-validation.
% Leave-One-Out (LOO) is the least biased estimator \cite{conf/ijcai/95}
% and usually the most computationally expensive.


% The task of model selection is frequently done by the application of some kind of validation,
% preferably cross-validation for efficient data reuse.
% Within this context,
% 
% Several fast cross-validation techniques have been proposed in the literature for different classifiers.
% Least-Squares Support Vector Machines leave-one-out in linear time complexity
% \cite{journals/nn/CawleyT04}.
% Classifiers whose model space forms a monoid or a group and whose batch trainer is a
% homomorphism \cite{conf/icml/Izbicki13}.
% ELM-FLOO has time complexity $mathcal{O}(n)$ \cite{xue2011fast}.
% 
% Some classifiers can be trivially used for incremental/decremental learning, e.g. Naive Bayes  (NB) and 5-NN \cite{conf/ecml/Lewis98,journals/tit/Hart68}
% 4.5 and Very Fast Decision Trees (VFDT) \cite{books/mk/Quinlan93,conf/kdd/DomingosH00} it is not trivial.


% No caso específico da ELM, há trabalhos que propõem a definição automática de $L$,
% porém com a introdução de um novo parâmetro \citep{journals/ijon/YuHMNHSL14}
% % DT-ELM adjusts $L$ automatically,
% % but depends on a new parameter, the number of clusters \cite{journals/ijon/YuHMNHSL14};
% (DT-ELM\footnote{As siglas das diversas variantes de ELM são apresentadas
% apenas para facilitar sua identificação. Os nomes por extenso não são relevantes
% para o escopo deste trabalho.})
% % E-ELM searches through a population of ELMs using the accuracy on a \textit{validation set}
% % as measure of fitness \cite{journals/pr/ZhuQSH05};
% E-ELM busca pela rede mais apta dentre uma população baseando-se na acurácia
% num conjunto de validação \cite{journals/pr/ZhuQSH05};
% % http://citeseerx.ist.psu.edu/viewdoc/download?rep=rep1&type=pdf&doi=10.1.1.217.3643
% I-ELM e EI-ELM dependem de um erro de treinamento pré-definido ou um número máximo
% de iterações como critério de parada \cite{journals/tnn/HuangCS06};
% % $\mu$G-ELM also depends on the \textit{mean square error} \cite{journals/ijon/LahozLM13};
% % http://www.sciencedirect.com/science/article/pii/S0925231213000520
% $\mu$G-ELM é um algoritmo genético multiobjetivo que tem como critério
% o erro quadrático médio \cite{journals/ijon/LahozLM13};
% http://research.ics.aalto.fi/eiml/Publications/Publication157.pdf
% OP-ELM methodology has three steps: (1) Build the SLFN using the original ELM
% algorithm, (2) Rank the hidden nodes by applying statistical criteria Chi-squared
% multi-response sparse regression
% algorithm (MRSR) [11], (3) Select the hidden nodes through leave-one-out
% OP-ELM \cite{journals/tnn/MicheSBSJL10} adota a medida PRESS
% \ano{passar press pra antes desta seção?} e ranqueia os neurônios por um critério estatístico
% \cite{mye:book};
% HQ-OP-ELM uses the non-exact Hannan-Quinn criterion which is faster than PRESS for very large datasets \cite{conf/esann/MicheL09};
% HQ-OP-ELM opta pelo critério não exato de Hannan-Quinn que é mais
% rápido que a PRESS para bases muito grandes \cite{conf/esann/MicheL09};
% http://research.ics.aalto.fi/eiml/Publications/Publication119.pdf
% EnhancedOS-ELM uses a complex set of three criteria including the Root Mean Squares value of the error over a sliding window
% (enhanced) EOS-ELM 2008
% http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4597855&tag=1
% for SLFNs with RBF hidden nodes based on minimal resource allocation network (MRAN)
% algorithm. Although we can get more compact network structure using EOS-ELM in the
% end, the method is complicate and only suitable for RBF hidden nodes (texto do ceos)).
% CEOS-ELM provides the optimal hidden nodes according to the \textit{expected error}
% given by the user \cite{conf/ijcnn/LanSH09};
% Experimentos: N0 começa grande (abalone=75, 200, 500, 800); Lmax em torno de ? de N0;
% fórmula 13 é central na atualização de Beta pro EM-ELM.
% http://ieeexplore.ieee.org/xpls/icp.jsp?arnumber=5178608 tem latex
% http://ieeexplore.ieee.org/xpl/login.jsp?tp=&arnumber=5178608&url=http%3A%2F%2Fieeexplore.ieee.org%2Fxpls%2Fabs_all.jsp%3Farnumber%3D5178608
% http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5161346&tag=1
% experimentos: sigmoide, normalizado entre [-1,1] (atts) e [0,1] (output); alguns datasets com aição de neurônios de 25 em 25 e outros de 1 em 1; target accuracy arbitrariamente setada em 90%.
% Huang et al. [6] has further shown that if the N training data are distinct, H is column full rank with probability one when L<=N.
% In real applications, one need not request that the network output
% error E(Hk ) is less than the target error . In fact, the network growing
% procedure could stop if the network output error reduced very slowly,
% one may also apply cross-validation methods in learning of EM-ELM.
% rank H = L, ou seja, duas condiçoes de acordo com a definição de rank MxN mais abaixo.
% 1) L <= N0  (pois o rank é sempre da menor dimensão)
% 2) todas colunas de H são linearly independentes*
% http://www.cds.caltech.edu/~murray/amwiki/index.php/FAQ:_What_does_it_mean_for_a_non-square_matrix_to_be_full_rank%3F
% So if there are more rows than columns (), then the matrix is full rank if the matrix is full column rank.
% Full-rank MxN = full [column|row] rank = todos independentes = [M|N]

% EnsembleOS-ELM
% CS-ELM depends on the \textit{training Mean Squared Error} \cite{journals/ijon/LanSH10a};
% TS-ELM relies on the final prediction error criterion which compromises the \textit{validation error} and $L$ \cite{journals/ijon/LanSH10};
% http://www.sciencedirect.com/science/article/pii/S0925231210003401
% http://ieeexplore.ieee.org/xpls/icp.jsp?arnumber=4746040
% http://download.springer.com/static/pdf/722/chp%253A10.1007%252F978-3-642-14922-1_2.pdf?auth66=1393101679_7d2915b6efac3e939585e1273dab21f4&ext=.pdf

% ELM-TV requires a predefined \textit{expected error} \cite{conf/apccas/CingolaniSP08};
% AIE-ELM requires the definition of the \textit{expected learning accuracy} \cite{conf/ais2/ZhangLHS11};
% AG-ELM selects the neurons that achieve the \textit{best generalization performance} \cite{journals/tnn/ZhangLHX12};
% http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=6125249

% D-ELM (a versão HTML tem os TEX sources das fórmulas da ELM) 2013
% Dynamic Extreme Learning Machine and Its Approximation Capability
% http://ieeexplore.ieee.org/xpls/icp.jsp?arnumber=6459569
% The key idea of D-ELM is the following. The network starts with one random hidden unit. Then, at each subsequent step, several new networks with different numbers of hidden nodes are first generated. Among all these new networks and the network obtained at the previous step, the generalization performance will be compared, and the ?best? one, whose number of hidden nodes is the least among those having the best generalization, will be selected. Adding one more random hidden node to such selected network then results in the network for the current step. In D-ELM, the hidden nodes are randomly generated, and the output weights are updated incrementally by using the error-minimization-based method proposed in [6].
% ...
% Comparing with those constructive ELMs, AG-ELM has two features: 1) the hidden-node number of AG-ELM is determined in an adaptive way in the sense that the existing networks may be replaced by some newly generated networks with better performance rather than always keeping those existing ones; 2) a group of new networks are generated at each step in AG-ELM (i.e.,  networks with different numbers of hidden nodes are generated at the th step), while in other constructive ELMs, only one new network is generated at each step.

% Comparativo com 10 variantes de ELM 2013, citar pra dizer que existem elms pra outras finalidades
% Extreme learning machine and its applications

% % % OPIUM (Online PseudoInverse Update Method) 2013
% % % Learning the pseudoinverse solution to network weights
% % % http://www.sciencedirect.com/science/article/pii/S089360801300049X
% % % We present an online or incremental method of computing the pseudoinverse precisely, which we argue is biologically plausible as a learning method, and which can be made adaptable for non-stationary data streams. The method is significantly more memory-efficient than the conventional computation of pseudoinverses by singular value decomposition.
% % % Neural Engineering Framework ( Eliasmith & Anderson, 2003) in computational neuroscience and the Extreme Learning Machine (ELM) ( Huang, Zhu, & Siew, 2006) in the machine learning field, both synthesize three-layer feedforward networks which are superficially similar to the classic multilayer perceptron ... What makes these architectures unique is that the input layer signals are connected to an unusually large number of hidden layer neurons, using randomly initialized connection weights. This has the effect of randomly spreading or projecting the inputs from their original input dimensionality to a hidden layer of very much higher dimensionality. It is then possible to find a hyperplane in the higher dimensional space which approximates a desired function regression solution, or represents a classification boundary for the input?output relationship.
% % % We will refer broadly to this class of methods as linear solutions to higher dimensional interlayer networks (LSHDI).
% % % structures embodying the LSHDI principle may exist in the brain. For example, recent work by Rigotti, Ben Dayan Rubin, Wang, and Fusi (2010) in modeling recorded cortical activity in monkeys performing context-sensitive tasks shows that complex rule-based tasks require both sensory stimuli and internal representation of states; and that a significant number of random connections placed between input sources and a hidden interlayer, and random recurrent connections between interlayer neurons, are necessary for optimal performance. They describe these interlayer neurons as generating mixed selectivity, which is equivalent to increasing the dimensionality of the state representation.
% % % An obstruction to acceptance of the LSHDI method as being biologically plausible is the necessity to compute the pseudoinverse of a matrix.
% % % The solution to the network weights is exactly the same as that which would be calculated by singular value decomposition. It converges with a single forward iteration per input data sample, and as such is ideal for real-time online computation of the pseudoinverse solution.
% % %  It requires significantly less memory than the SVD method, as its memory requirement scales as the square of the size of the hidden layer, whereas the SVD memory requirement scales with the product of the hidden layer size and the size of the training data set.
% % % OPIUM is adapted from an iterative method for computing the pseudoinverse, known as Greville?s method (Greville, 1960).
% % % Pseudoinverse methods were widely used in an earlier era of neural network research, to the extent that a significant class of Kohonen-type linear associative memories were known as pseudoinverse neural networks (PINNs) (Kohonen, 1988 and Kohonen, 1989). The key variations in the LSHDI use of the method are in the initial spreading to higher dimension, and the associated nonlinear activation performed on the higher dimension signals.
% % %  typical test error on the raw MNIST data set is 2.75%
% % % apart from the number of hidden-layer neurons, there are no parameters to be tuned

% Fast sparse approximation of extreme learning machine(2014)
% http://www.sciencedirect.com/science/article/pii/S0925231213010047
% the solution of LS-SVM lacks sparseness and hence, the test speed is significantly slower than that of other algorithms, such as the support vector machine (SVM) [5] and [6] and neural networks
% ELM: unified learning mode for regression, binary, and multiclass classification
% aparentemente somente para kernels (talvez seja um restrição inerente à tarefa de se criar uma ?sparse solution?.
% The popular Gaussian kernel function K(u,v)=exp(??2?u?v?) is used in SVM, LS-SVM, SLFN-ELM, and FSA-ELM. In order to achieve a good generalization performance, the cost parameter C and kernel parameter ? of SVM, LS-SVM, and SLFN-ELM, need to be chosen appropriately.

% TOSELM: Timeliness Online Sequential Extreme Learning Machine 2014
% http://www.sciencedirect.com/science/article/pii/S0925231213009120
% Timeliness here means the data distribution or the data trend changes with time passing by.

% Parsimonious Extreme Learning Machine Using Recursive Orthogonal Least Squares (2014):
% % http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=6704311
% Castaño et al. [17]
% proposed a robust and pruned ELM approach based on prin-
% cipal component analysis (PCA), termed PCA-ELM, whereby
% the PCA method is used to select the hidden nodes from input
% features while corresponding input weights are deterministi-
% cally defined as principal components rather than random ones.
% Note that the deterministic approach to hidden node selection
% inevitably results in poor accuracy especially for noisy data.
% % http://en.wikipedia.org/wiki/Multicollinearity#Remedies_for_multicollinearity :
% Note that one technique that does not work in offsetting the effects of multicollinearity is orthogonalizing the explanatory variables (linearly transforming them so that the transformed variables are uncorrelated with each other): By the Frisch?Waugh?Lovell theorem, using projection matrices to make the explanatory variables orthogonal to each other will lead to the same results as running the regression with all non-orthogonal explanators included.

% Comparando com SVM 2012:
% http://www.ntu.edu.sg/home/egbhuang/pdf/ELM-Unified-Learning.pdf
% http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.298.1213&rep=rep1&type=pdf
% ELM tends to have better scalability and achieve similar (for regression and
% binary class cases) or much better (for multiclass cases) generaliza-
% tion performance at much faster learning speed (up to thousands
% times) than traditional SVM and LS-SVM.
% ...
% According to the ridge regression theory [37], one can add
% a positive value to the diagonal of HT H or HHT ; the resul-
% tant solution is stabler and tends to have better generalization
% performance. Toh [22] and Deng et al. [21] have studied the
% performance of ELM with this enhancement under the Sigmoid
% additive type of SLFNs.
% ...
% In this specific case, similar to SVM, LS-SVM, and PSVM,
% the feature mapping h(x) need not be known to users;
% instead, its corresponding kernel K(u, v) (e.g., K(u, v) =
% exp(?? u ? v 2 )) is given to users. The dimensionality L of
% the feature space (number of hidden nodes) need not be given
% either.

% Architecture selection for networks trained with extreme learning machine using localized generalization error model 2013
% http://www.sciencedirect.com/science/article/pii/S0925231212004328
% só pra RBF.
% If the number of hidden nodes is equal to the number of distinct training samples, the matrix H is square and invertible, and SLFNs can approximate these training samples with zero error.

% D-ELM depends on the \textit{generalization performance} \cite{journals/tcyb/ZhangLHXS13};
% FSA-ELM changes $L$ indirectly by an additional parameter \cite{journals/ijon/LiMJ14};

% There is one SVM-based approach in which $L$ is not critical,
% but it is outside the scope of this paper \cite{conf/esann/FrenayV10}.

% meta
% Another approach is to explore the fact that $L$ is dataset-dependent.
% It is possible to use \textit{meta-learning} to discover the relations between
% meta-attributes and $L$.
% This was proposed recently in the literature,
% with $L$ in the range $[1,300]$ for 93 regression problems \cite{conf/cbic/lucas11}.
% An already trained meta-classifier can be used to perform a warm start,
% limiting the search space for model selection.
% However,
% both this step of refining the search and the very expensive previous step of
% training the meta-classifier require an efficient and immune-to-overfitting validation process.

% CEOS-ELM addresses both the weights incremental update due to new instances arrival
% and the adequate growing of the hidden layer.
% However, from the original paper,
% it is possible to note that the convergence (to the original ELM) requires a certain amount
% of instances for the initial training.
% Apparently, the advantage of CEOS-ELM is to reduce training time in large training sets
% by avoiding recalculation of the pseudo-inverse of H from scratch at every time step.
% se CEOS comecar a partir de $L=1$, entao eh aplicavel a model selection (soh em
% batch learning por eventuais problemas de convergencia quando se comeca com poucos
% exemplos).

% Like in the other ELM variants,
% it is impossible to determine the target accuracy nor to define confidently $L_0$,
% $L_{max}$ and $N_0$ beforehand.
% Also, $L_{max}$ is around a third of $N_0$ which is impossible for small $N_0$s.









% http://research.ics.aalto.fi/eiml/Publications/Publication208.pdf
% adjusts $L$ automatically, but depends on a new parameter, the number of clusters
% 
% AIE-ELM 2011
% Extreme Learning Machine with Adaptive Growth of Hidden Nodes and Incremental Updating of Output Weights
% http://link.springer.com/chapter/10.1007%2F978-3-642-21538-4_25
% AIE-ELM grows the randomly generated hidden nodes in an adaptive way in the sense that the existing hidden nodes may be replaced by some newly generated hidden nodes with better performance rather than always keeping those existing ones in other incremental ELMs. The output weights are updated incrementally in the same way of error minimized ELM (EM-ELM). Simulation results demonstrate and verify that our new approach can achieve a more compact network architecture than EM-ELM with better generalization performance.

% Por fim, há propostas que buscam pelos melhores pesos da camada oculta,
% entretanto passa-se a se tratar de um desvio da ideia original mais próximo
% das redes neurais convencionais e sua complexidade de configuração do que
% da simplicidade da ELM original.
% Fixado o tipo de nó, os únicos parâmetros a ajustar a quantidade de neurônios na camada oculta
% e os valores de suas conexões sinápticas.
% Dada uma função geradora de pesos aleatórios ou pseudoaleatórios e sua semente geradora,
% a quantidade de
% neurônios $L$ na camada oculta pode ser vista como o único parâmetro a se ajustar.
% Entretanto, há evidências de que a projeção no espaço de maior dimensionalidade
% realizada pela camada oculta aleatória é inferior à realizada por uma MLP,
% por exemplo (\citep{parviainen2011studies}).
% https://aaltodoc.aalto.fi/bitstream/handle/123456789/5067/isbn9789526043128.pdf?sequence=1
% The only theoretically motivated upper limit for the number of hidden units to try is N (which is enough for zero training error).
% ...
% At that limit, computing pseudoinverse corresponds to ordinary inversion of an N × N matrix, with a complexity of O(N3). In practice, smaller upper limits are used.
% ...
% Aqui tem uns desenhos legais mostrando que a projeção aleatória da ELM
% no espaço de maior dimensão é algo que pode ser melhorado na página 47.

% Esse parâmetro frequentemente não é crítico \citep{conf/esann/FrenayV10}.
% http://www3.ntu.edu.sg/home/EGBHuang/pdf/ELM_IJCNN2004.PDF \citep{huang2004extreme}
% simulation results in artificial and real world cases done in [7] as well as in this paper further
% demonstrated that no gain is possible by adjusting the input weights and the hidden layer biases

% * A partir de um certo momento, a camada oculta passou a ser tratada como um
% conjunto de paraâmetros e não apenas seu tamanho. Daí surgiram as propostas
% evolucionárias/evolutivas(?) em oposição às propostas de crescimento da camada
% e/ou de atualização dos pesos.