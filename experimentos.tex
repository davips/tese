%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Experimentos e Resultados}\label{experimentos}

\section{Algoritmos de aprendizado}
Uma forma de ilustrar a diversidade entre os modelos gerados
pelos diferentes algoritmos de aprendizado é por meio da similaridade
entre suas acurácias.
Ela é apresentada na Tabela \ref{passiveDists} de forma análoga à adotada por
\cite{brazdil1994analysis}.
\input passiveDists
% faz tabela de distâncias pareadas com taxas de erro sendo
% as dimensões, depois faz decomposição ortogonal para visualizar em duas dimensões.
% Depois ele faz agrupamento hierárquico.

Esses modelos são o resultado de treinamento passivo, ou seja, com todos os
exemplos da reserva.
Considerando-se cada base de dados uma dimensão,
a similaridade é calculada a partir da distância euclidiana $d(\bm{x},\bm{u})$
entre dois vetores $\bm{x},\bm{u} \in \mathbb{R}^{94}$
de valores de acurácia balanceada de acordo com a mesma fórmula
(\ref{eq:sim}) adotada no cálculo de densidade da Seção \ref{dw}.

Conforme esperado,
a maior similaridade ($0,77$) ocorre entre IELM e CIELM.
Outros valores relevantes são as proximidades entre VFDT e NB ($0,56$)
e entre 5NN e C4.5 ($0,52$).
O algoritmo mais distinto é a SVM, pois apresenta os menores valores em relação a
todos os demais (entre $0,28$ e $0,30$).
Dessa forma, uma definição de quatro grupos pode ser útil durante a análise
da influência do classificador nas estratégias de aprendizado ativo:
IELM-CIELM, VFDT-NB, 5NN-C4.5 e SVM.

\ano{é preciso adicionar nintera e zeroR nas dists, nas 3 tabelas e no fried (e atualizar texto);
eles ainda estão rodando}
As acurácias para cada base são apresentadas nas tabelas \ref{tab:balaccClassif0},
\ref{tab:balaccClassif1} e \ref{tab:balaccClassif2}.

Na comparação um a um
\input classifsTabFried


\ano{selecionar das variantes da literatura fazendo mini friedman
para cada tipo: unc,ent,mar  eer   dwtus ou usar centroide por distEntrePassivas?}

x Qual a economia proporcionada por uma estratégia?

x Quão bem uma estratégia usufrui de um dado orçamento?

AG,LU vs DWTU

\tar{há alguma correlação entre desempenho de AL e desbalanceamento da base?}

\input exp-analise





 \tar{usar nintera na(s) base(s) à parte?
 ou usar em todas, mas sem strats lentas?}

 \tar{plotar}



\green{baseline p/ AccBal é $1/|Y|$}

\green{manter apenas estratégias que ganharam em algum momento/nicho?}
\blue{assim é possível o avaliar estratégias num cenário em que se sabe de
antemão qual é o melhor learner.
manter todas as combinações learner-dataset serve para o cenário em que
nada se sabe do dataset, ou seja, é o mais esperado,
pois normalmente é preciso ter rótulos suficientes para se testar classificadores;
além disso, quando se conhece o melhor classificador
(a não ser por propriedades dos dados:
ex.: nominal->NB, muitos atributos->SVM etc.)
é por meio de um problema parecido onde as estratégias podem ser testadas}

\blue{plotar (log?) rnd (e clu?) p/ nb,c45 e 5nn até o fim do pool ou
até atingir passiva, assim
serve de panorama do qto as outras estrategias/learners
atingiram da ALC possivel)}

\green{rodar eeg-eye-state pra medir tempo nas principais estrategias/learners,
pra ser a unica base usando nintera (nintera come memória);
rodar como todas as outras, mas em 1x10-fold}
\red{o tempo ficou sempre abaixo de 1s, então está dentro do tolerable
waiting time.
Suponho que a demora do experimento se deva à inicialização das strats
e não às consultas propriamente ditas.}

\subsubsection{Afinidade estratégia-aprendiz, estratégia-budget e outros nichos}

\ano{A primeira árvore é aplicada a todos os datasets; ele servirá apenas para dar algum insight
sobre o motivo de cada afinidade existir (desbalanceamento, agrupamentos, tipos e qtd de atts etc.).
Não serve para ML, pois os atributos e o
claffisifcador devem ser interpretáveis e haverá atributos com informação ``desleal''
da distribuição de classes}

\ano{possiveis atts: proporção da minor., da maj.; entropia* ou medida melhor para medir o grau de desbalanço}

* -> normalized entropy \citep{journals/bioinformatics/LewinSA0P04}:
ajuda a saber que estratégias são boas p/ bases desbalanceadas.

----------

\green{árvore do vencedor accbal e variancia}

\green{árvore do perdedor accbal e variancia (considerar empates? = uma árvore por estrat.?)}

\subsubsection{Comparativo}
\green{friedmenyi de  accbal e variancia}

\green{plotar accBal X budgets p/ cada uma das 15 bases com menos de
200 exemplos?}

\green{plotar p/ accBal (apenas nas 85 bases com mais de
200 exemplos?)): \#topos-de-rank X budgets $[10;200]$
Dois tipos de \#topos-de-rank:
com empate = \#vezes em que ficou entre os vencedores
sem empate = \#vezes em que foi o vencedor}

\subsubsection{Aprendizado meta-ativo}
\ano{ainda é preciso ver se ranking supera ranking medio (default)}

\ano{possibilidades: um experimento para cada learner separadamente;
sortear budget no exp acima;
sortear learners num experimento só;}

