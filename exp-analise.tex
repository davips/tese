\section{Análise comparativa}\label{comparativa}
Há aplicações em que suas particularidades ou preferências do especialista determinam
previamente o algoritmo de aprendizado. \ano{exemplos?}
Por outro lado, há aplicações, possivelmente a maioria, em que a escolha é livre
e concretizada idealmente a partir do momento em que o conjunto de treinamento é
suficientemente grande para a realização de uma validação cruzada \ano{ref?}.
Para simular esses dois cenários de forma concisa e
devido à presença de estratégias específicas de um único tipo de aprendiz
(QBC, SVMS e SVMB),
% (QBC, EMC, SVMS e SVMB),
a comparação de estratégias está dividida em três partes:
\begin{itemize}
 \item \textbf{comparação com algoritmos variados} - compreende algoritmos não vinculados a nenhuma estratégia
 (5NN, C4.5w, CIELM e NB); incidentalmente, esse critério remove os algoritmos que aparecem
 nos extremos da contagem de posições, deixando a contagem de primeiros
e últimos lugares mais equilibrada, entre $21$ e $27$ e entre $19$ e $28$, respectivamente;
 \item \textbf{comparação RFw} - por ser um comitê, o RF viabiliza o uso da estratégia QBC;
  nessa comparação é possível avaliar a existência de benefícios em aplicá-la frente
  às estratégias de uso geral; é possível também observar quais estratégias são adequadas
  para esse algoritmo;
  \item \textbf{comparação SVM} - de forma análoga à \textit{comparação RFw},
  porém com duas estratégias específicas; e,
%   \item \textbf{comparação ELM} - de forma análoga à \textit{comparação RFw},
%   porém com o diferencial de revelar de forma pioneira a viabilidade de se aplicar
%   aprendizado ativo e a estratégia específica com ELMs.
  \end{itemize}

\subsection{Comparação com algoritmos variados}\label{cdiversa}
% http://research.cs.wisc.edu/techreports/2009/TR1648.pdf
Estratégias são normalmente avaliadas por meio de curvas de aprendizado,
que são os gráficos da medida de interesse em função da quantidade de consultas
\citep{settles2010active}.
% Figure 3 presents learning curves for the first 100 instances labeled using
% uncertainty sampling and random sampling.
% The reported results are for a logistic regression model averaged over ten folds
% using cross-validation.
O comportamento típico da curva de aprendizado ativo seguido por todas as diferentes estratégias
é ilustrado na Figura \ref{curvas}
Apesar da pouca discernibilidade das curvas, a exibição delas para todas as estratégias permite notar
que elas têm a mesma forma logarítmica, com o valor kappa divergindo na medida em que novos
exemplos são consultados.
\input plot
Se todas as bases tivessem a mesma quantidade de exemplos
e o aprendizado prosseguisse, as curvas se encontrariam novamente,
pois tendem ao desempenho passivo.

É possível observar que a curva da DWeuc se destaca negativamente,
mas comparações de curvas neste gráfico são imprecisas devido aos diferentes
pesos que as bases podem ter:
bases mais difíceis tendem a ter um valor mais baixo para kappa e acabam
sub-representadas.
Uma forma de neutralização da desigualdade de nível de dificuldade entre as bases é a adoção
de ranqueamento das médias\footnote{Além das médias,
um gráfico das medianas também foi elaborado e teve um comportamento praticamente igual.
% com leves perturbações irrelevantes
Por motivos de espaço ele foi omitido.}, conforme mostrado na Figura \ref{curvasrank}.
\input rankplot
O fato de nem todas as estratégias agnósticas terem sido favorecidas sugere que a ausência
de premissas baseadas no viés do aprendiz não é condição preponderante para um bom
desempenho em algoritmos de aprendizado variados, como  Rnd e Clu que permaneceram
debaixo da colocação média ($5.5$) por praticamente todo o intervalo de consultas.

Conforme esperado, DWeuc teve o pior desempenho por concentrar as consultas apenas
nas regiões mais densas e de fronteira, deixando de explorar adequadamente o espaço de exemplos.
DWeuc é a estratégia Mar ponderada pela densidade, logo, dada a superioridade de Mar,
a ponderação por densidade pode ser considerada bastante prejudicial quando não leva em
conta os exemplos rotulados.
Por outro lado, as extensões de DW (TU, ATU e GATU) superam as demais, pois ponderam inversamente pela
densidade de exemplos rotulados evitando, assim, as regiões densamente rotuladas.
SGmulti, por sua vez, se manteve próximo à colocação média.
Um evento importante entre as melhores curvas é a inversão entre a estratégia gnóstica
TUmah e sua versão agnóstica ATUmah após aproximadamente $90$ exemplos.
Essa inversão coincide com a intuição de que consultas exploratórias sejam vantajosas no início do
aprendizado, enquanto que consultas prospectivas passem a ser proveitosas mais tardiamente.
Assim, uma estratégia como a GATUmah, capaz de migrar entre agnosticidade e gnosticidade,
pode ter um bom desempenho tanto no início quanto no restante da curva de aprendizado.
A ascensão de Mar também é um indicativo de que a prospecção é prejudicial no início
e vantajosa com o avanço do aprendizado.
A diferença entre EERacu e EERent é um indício de que atacar diretamente a função objetivo
não é tão eficaz quanto o uso da medida de entropia.
Nas análises seguintes, as estratégias DWeuc - por ser claramente superada pelas suas extensões - e
EERacu - por ser melhor representada pela proposta original da EER - foram omitidas.

Para confirmar as conclusões com segurança estatística, os níveis de confiança
($p \in \{0,01; 0,05; 0,10\}$), na Tabela \ref{stratsALCKappaFriedAllReduxall}, indicam
quando uma estratégia na linha é melhor que a outra na coluna.
O critério de vitória é baseado na comparação dos valores da ALC-kappa para todas as bases.
A medida foi calculada com $200$ exemplos nas bases de tamanho suficiente
e $|\mathcal{U}|$ nas demais.
% \input stratsmahALCKappaFriedAllReduxall
\begin{table}[h]
\caption{Um contra um para as estratégias (376 testes).
Medida: ALC-kappa. \textit{Legenda na Tabela \ref{tab:friedClassif}.}}

\begin{center}
\scalebox{0.94}{\begin{tabular}{lcc|cc|cc|cc}
                        & 1 & 2 & 3 & 4 & 5 & 6 & 7 & 8\\
1 - Rnd         & - &   &   &   &   &   &   &   \\
2 - Clu         &   & - &   &   &   &   &   &   \\ \hline
3 - \textbf{ATUmah}     & * & * & - &   & * &   &   &   \\
4 - \textbf{GATUmah}    & * & * &   & - & * & . &   &   \\ \hline
5 - \textbf{SGmulti}    & * &   &   &   & - &   &   &   \\
6 - Mar         & * & * &   &   & . & - &   &   \\ \hline
7 - TUmah       & * & * &   &   & * &   & - &   \\
8 - EERent      & * & * &   &   & * &   &   & - \\ \hline\end{tabular}
}
\quad
\scalebox{0.91}{\begin{tabular}{lccc}
algoritmo & \makecell{primeiros\\lugares} & \makecell{derrotas\\para Rnd}  & \makecell{últimos\\lugares} \\
\hline
EERent          &       \textcolor{blue}{\textbf{94}}      &       135     &       \textcolor{red}{\textbf{74}}      \\
TUmah           &       \textbf{80}      &       126     &       41      \\
Mar             &       72      &       129     &       \textcolor{red}{\textbf{74}}      \\
HTUmah          &       64      &       \textcolor{blue}{\textbf{106}}     &       \textcolor{blue}{\textbf{23}}      \\
ATUmah          &       49      &       \textbf{112}     &       \textbf{31}      \\
SGmulti         &       21      &       166     &       34      \\
Clu             &       19      &       \textcolor{red}{\textbf{168}}     &       56      \\
Rnd             &       \textcolor{red}{\textbf{8}}       &       -     &       \textcolor{red}{\textbf{74}}      \\
\end{tabular}}
\label{stratsALCKappaFriedAllReduxall}
\end{center}
\end{table}

Como um indicativo da viabilidade do aprendizado ativo em geral, Rnd perde de todas exceto Clu.
Ainda assim, o princípio teórico de Clu, que garante um desempenho que não seja pior que a
amostragem aleatória, manteve-se válido.
Dentre as melhores estratégias, a proposta GATUmah é a preferível, pois além de se equiparar
às outras ela foi superior à Mar - ainda que com baixo nível de confiança ($p=0,10$).
A menor quantidade de derrotas para Rnd e de últimos lugares são duas vantagens das duas propostas
baseadas na TUmah, pois indicam o melhor desempenho frente ao não uso de aprendizado ativo
e um menor risco de se obter o pior desempenho.
SGmulti superou apenas a aleatória.

% Se forem consideradas apenas as primeiras 100 consultas -
% %150 conf/emnlp/SettlesC08
% valor arbitrário frequentemente adotado em outros trabalhos
% \citep{journals/pieee/CrawfordTY13,chermanaprendizado,conf/nips/SettlesCR07,conf/icml/RoyM01} -
% A comparação passa a ser como na Tabela \ref{stratsALCKappaFriedAllReduxHalf}.
% \input stratsmanALCKappaFriedAllReduxall
% \input stratseucALCKappaFriedAllReduxall
% conforme Tabela \ref{stratsALCKappaFriedAllReduxHalf}.
% $50$ exemplos: quantidade máxima antes que o tamanho das bases permita ALCs de tamanhos diferentes
% - conforme Tabela \ref{stratsALCKappaFriedAllRedux50}.
Em aplicações reais, entretanto, a estabilidade do algoritmo pode ser mais importante
do que o desempenho preditivo médio, pois uma grande variabilidade pode colocar o
orçamento sob o risco de ser gasto sem que seja atingido um desempenho adequado e pode levar a custos
de rotulação imprevisíveis.
Assim, a estabilidade da estratégia tem fundamental importância para a análise do risco financeiro
da aplicação. É importante enfatizar que o processo de rotulação ocorre apenas uma vez em uma
dada reserva de exemplos até o término do orçamento, impossibilitando novas tentativas.

De forma análoga às curvas apresentadas para o valor médio de kappa e o ranqueamento
correspondente, dois conjuntos de curvas baseadas no desvio padrão do valor de kappa foram
construídos visando ilustrar o comportamento da variância preditiva a cada instante ao longo
de sucessivos valores de orçamento.
É possível observar, na Figura \ref{curvasdesvio}, que as estratégias ATUmah e GATUmah
seguidas de TUmah detêm os menores valores de desvio padrão durante a maior parte do
processo de rotulação.
\input plotRisco
O mesmo ocorre com os ranqueamentos apresentados na Figura \ref{curvasrankdesvio},
porém com EERent alternando com TUmah.
\input rankplotRisco
A curva de SGmulti ficou abaixo do ranqueamento médio ($4,5$).
Uma propriedade das curvas é a clara tendência de estabilização durante o treinamento,
indicando que em baixos orçamentos a escolha de estratégias mais estáveis é prioritária.
A significância estatística das diferenças pode ser verificada na Tabela
\ref{stratsALCKappaFriedAllRiscoReduxall}.
% \input stratsmahALCKappaFriedAllRiscoReduxall
\begin{table}[h]
\caption{Um contra um para as estratégias. Medida: desvio padrão da ALC-kappa. \textit{Legenda na Tabela \ref{tab:friedClassif}.}}
\begin{center}\begin{tabular}{lcc|cc|cc|cc}
                                 & 1 & 2 & 3 & 4 & 5 & 6 & 7 & 8\\
1 - Rnd                       & - &   &   &   &   &   &   &   \\
2 - Clu                         &   & - &   &   &   &   &   &   \\ \hline
3 - \textbf{ATUmah}     & * & *  & - &   & *  & * &  & *   \\
4 - \textbf{GATUmah}    & * & * &   & - & * & * &   & * \\ \hline
5 - \textbf{SGmulti}    &   &   &   &    & - &   &   &   \\
6 - Mar                         &   &   &   &   &   & - &   &   \\ \hline
7 - TUmah                       & + & * &   &   & + & * & - & . \\
8 - EERent                      &   &   &   &   &   &   &   & - \\ \hline\end{tabular}
% calculado pelo R
\label{stratsALCKappaFriedAllRiscoReduxall}
\end{center}
\end{table}

Finalmente, pode-se concluir que, segundo o conjunto de bases e algoritmos utilizados,
as versões agnóstica e híbrida propostas (ATUmah e GATUmah) são viáveis enquanto alternativas
à TUmah, com pequena vantagem para GATUmah em acurácia quando considerada
a quantidade de estratégias estatisticamente inferiores a elas.
Com relação à minimização de risco financeiro, 
novamente as estratégias propostas também se mostraram viáveis,
vencendo as mesmas estratégias que TUmah, porém com maior significância estatística.

\subsection{Comparação RFw}\label{crf}
A estratégia específica QBC perdeu de todas, exceto GATUman e Rnd,
conforme Tabela \ref{stratsALCKappaFriedRFwRedux}.
GATUman também teve um desempenho significativamente negativo.
Com o algoritmo RFw, as estratégias mais exploratórias ficaram em desvantagem,
com TUman vencendo todas, exceto Mar, que é uma estratégia puramente
prospectiva.
SGmulti, Mar e EERent também tiveram algum destaque, em oposição ao ocorrido
na comparação de estratégias com algoritmos variados.

\begin{table}[h]
\caption{Um contra um (RFw). Medida: ALCKappa. \textit{Legenda na Tabela \ref{tab:friedClassif}.}}
\begin{center}
\scalebox{0.9}{\begin{tabular}{lcc|cc|cc|cc|c}
                        & 1 & 2 & 3 & 4 & 5 & 6 & 7 & 8 & 9\\
1 - Rnd         & - &   &   &   &   &   &   &   &   \\
2 - Clu         &   & - &   &   &   &   &   &   & * \\ \hline
3 - \textbf{ATUmah}     & * &   & - & * &   &   &   &   & * \\
4 - \textbf{HTUmah}    &   &   &   & - &   &   &   &   &   \\ \hline
5 - \textbf{SGmulti}    & * & * &   & * & - &   &   &   & * \\
6 - Mar         & * & * & * & * &   & - &   &   & * \\ \hline
7 - TUmah       & * & * & * & * & * &   & - & * & * \\
8 - EERent      & * & * &   & * &   &   &   & - & * \\ \hline
9 - QBC         &   &   &   &   &   &   &   &   & - \\\end{tabular}
\quad
\begin{tabular}{lccc}
algoritmo & \makecell{primeiros\\lugares} & \makecell{derrotas\\para Rnd}  & \makecell{últimos\\lugares} \\
\hline
TUmah      & \bom{50} & \bomd{13} & 2 \\
Mar        & \bomd{19} & 14 & 2 \\
ATUmah     & 9 & 18 & 5 \\
SGmulti    & 8 & \bom{8} & \bom{0} \\
EERent     & 3 & 14 & 2 \\
Rnd        & 2 & - & 24 \\
QBC        & 1 & \ruim{64} & \ruim{45} \\
HTUmah     & 1 & 48 & 13 \\
Clu        & \ruim{1} & 25 & \bomd{1} \\
\end{tabular}
}
\label{stratsALCKappaFriedRFwRedux}
\end{center}
\end{table}


\subsection{Comparação SVM}\label{csvm}
Diferentemente do que ocorreu com RFw, a SVM favoreceu GATUman.
EERent apareceu como a melhor opção, vencendo praticamente todas.
As estratégias específicas não tiveram tantas derrotas quanto a QBC na Seção \ref{crf}
\begin{table}[h]
\caption{Um contra um (SVM). Medida: ALCKappa. \textit{Legenda na Tabela \ref{tab:friedClassif}.}}
\begin{center}
\scalebox{0.85}{\begin{tabular}{lcc|cc|cc|cc|cc}
                        & 1 & 2 & 3 & 4 & 5 & 6 & 7 & 8 & 9 & 10\\
1 - Rnd         & - &   &   &   &   &   &   &   &   &   \\
2 - Clu         &   & - &   &   &   &   &   &   &   &   \\ \hline
3 - \textbf{ATUmah}     &   &   & - &   &   &   &   &   &   &   \\
4 - \textbf{HTUmah}    & * &   & . & - & * & * &   &   & * & + \\ \hline
5 - \textbf{SGmulti}    &   &   &   &   & - &   &   &   &   &   \\
6 - Mar         &   &   &   &   &   & - &   &   &   &   \\ \hline
7 - TUmah       & . &   &   &   &   &   & - &   &   &   \\
8 - EERent      & * & * & * &   & * & * & * & - & * & * \\ \hline
9 - SVMRBFbal   &   &   &   &   &   &   &   &   & - &   \\
10 - SVMRBFsim  &   &   &   &   &   &   &   &   &   & - \\ \hline\end{tabular}
\quad
\begin{tabular}{lccc}
algoritmo & \makecell{primeiros\\lugares} & \makecell{derrotas\\para Rnd}  & \makecell{últimos\\lugares} \\
\hline
EERent     & \bom{32} & \bom{19} & \bom{2} \\
SVMsim     & \bomd{17} & 48 & \ruim{23} \\
HTUmah     & 13 & \bomd{26} & 5 \\
SVMbal     & 8 & \ruim{51} & 13 \\
TUmah      & 7 & 36 & 5 \\
Clu        & 7 & 34 & 7 \\
Mar        & 4 & 44 & 18 \\
ATUmah     & 4 & 35 & 6 \\
SGmulti    & 2 & 42 & \bomd{4} \\
Rnd        & \ruim{0} & - & 11 \\
\end{tabular}
}
\label{stratsALCKappaFriedSVMRedux}
\end{center}
\end{table}


% \subsection{Comparação ELM}\label{celm}
% Similarmente ao ocorrido com a QBC, a estratégia específica da ELM (EMC)
% perdeu de todas incluindo a amostragem aleatória.
% Isso sugere que a proposta não é viável.
% Porém, segundo a comparação de classificadores da Seção \ref{algs},
% a ELM teve apenas quatro primeiros lugares dentre as $94$ bases de dados.
% É possível que a seleção de modelos via PRESS tenha influenciado negativamente
% as predições e consequentemente prejudicado a estimativa de mudança no modelo.
% TUmah, EERent e Mar venceram as três estratégias agnósticas com diferentes
% graus de significância estatística.
% SGmulti superou apenas as estratégias Rnd e Clu.
% \begin{table}[h]
% \caption{Um contra um (ELM). Medida: ALCKappa. \textit{Legenda na Tabela \ref{tab:friedClassif}.}}
% \begin{center}\begin{tabular}{lcc|cc|cc|cc|c}
%                         & 1 & 2 & 3 & 4 & 5 & 6 & 7 & 8 & 9\\
% 1 - Rnd         & - &   &   &   &   &   &   &   & + \\
% 2 - Clu         &   & - &   &   &   &   &   &   & + \\ \hline
% 3 - \textbf{ATUmah}     &   &   & - &   &   &   &   &   & * \\
% 4 - \textbf{GATUmah}    &   &   &   & - &   &   &   &   & * \\ \hline
% 5 - \textbf{SGmulti}    & + & + &   &   & - &   &   &   & * \\
% 6 - Mar         & * & * & . &   &   & - &   &   & * \\ \hline
% 7 - TUmah       & * & * & + &   &   &   & - &   & * \\
% 8 - EERent      & * & * & + &   &   &   &   & - & * \\ \hline
% 9 - EMC         &   &   &   &   &   &   &   &   & - \\\end{tabular}
% \quad
% \begin{tabular}{lccc}
% algoritmo & \makecell{primeiros\\lugares} & \makecell{derrotas\\para Rnd}  & \makecell{últimos\\lugares} \\
% \hline
% TUmah      & 28 & 36 & 3 \\
% EERent     & 18 & 25 & 3 \\
% Mar        & 15 & 29 & 6 \\
% EMC        & 8 & 69 & 58 \\
% SGmulti    & 7 & 24 & 2 \\
% HTUmah     & 7 & 38 & 7 \\
% ATUmah     & 5 & 37 & 7 \\
% Clu        & 5 & 45 & 4 \\
% Rnd        & 3 & 94 & 9 \\
% \end{tabular}
% \label{stratsALCKappaFriedELMRedux}
% \end{center}
% \end{table}

\subsection{Síntese das comparações}\label{sintese}
Estratégias agnósticas têm a vantagem de dispensar o aprendiz.
Assim, elas evitam o risco de uma escolha inadequada de algoritmo de aprendizado
para um dado problema.
Consequentemente, ATUmah seria a preferível no caso de algoritmos variados.
Entretanto, os resultados dos quatro experimentos apontam para diferentes melhores
estratégias.
Apesar de ATUmah não ter sido pior que a amostragem aleatória em nenhum deles,
ela perde de Mar, TUmah e EERent em alguns casos.
Logo, se for desejado o melhor desempenho para um dado algoritmo,
então é preciso recorrer a algum procedimento que minimize a chance de
uma escolha inadequada de estratégia.
Além do algoritmo, o orçamento disponível e as características de cada base também
podem favorecer ou prejudicar certas estratégias.

Nesta seção, as estratégias são comparadas com o objetivo de identificar nichos
em que umas possam se sobressair em relação às outras.
Cada nicho corresponde a um algoritmo e um conjunto de bases de dados similares entre si.

Nas árvores exibidas nas figuras \ref{treeBest} e \ref{tree},
é possível observar o papel central do algoritmo de aprendizado sobre
a estratégia de aprendizado ativo, pois é a primeira regra em ambas as árvores.
Elas simulam dois cenários com relação à escolha do algoritmo: quando o mais adequado é conhecido
(\ref{treeBest}) e quando ele é arbitrário (\ref{tree}).
O primeiro caso resulta em $95$ metaexemplos devido a um empate,
e o segundo caso também resulta em torno de $95$ metaexemplos, porém para cada algoritmo,
totalizando $595$.
% Apesar de ilustrativa, a árvore pode não ser segura do ponto de vista de tomada de
% decisão devido ao baixo grau de pureza das folhas.
\usetikzlibrary{trees}
\input arvorebest
\input arvore
\input arvorebestperd
\input arvoreperd

\ref{tree}
A amostragem aleatória aparece apenas na folha que representa o uso do 5NN nas quatro bases que têm
uma proporção de atributos nominais não muito alta (igual ou abaixo de $77$),
poucos atributos em geral (igual ou abaixo de $6$) e não muitos exemplos (igual ou abaixo de $1078$).
Assim, pode-se concluir que não optar pelo aprendizado ativo raramente é a melhor estratégia.
Num nicho similar, porém com muitos exemplos (acima de $1078$), HTUmah é a mais indicada.
ATUmah

\ref{treeBest}
O algoritmo NB produz estimativas de distribuição de probabilidade excessivamente confiantes
que podem ser amenizadas por um \ing{comitê por amostragem}{bagging} \citep{conf/icml/RoyM01}.
Estratégias que dependem dessas estimativas podem ser prejudicadas.
ATUmah por ser agnóstica é independente dessas estimativas e aparece como nó
folha para esse algoritmo de aprendizado.
EERent também aparece e é gnóstico,
mas seu uso da soma das entropias das estimativas de
probabilidade de todos os exemplos da reserva ameniza o efeito do excesso de confiança.

% Algumas ramificações aceitam explicações plausíveis:
% \begin{itemize}
%  \item
% 
% % \esb{When Does Active Learning Work?
% % confirma que atributos não-discretizados favorecem AL,
% % isso justifica z-score para qq classificador ?
% % (ao menos RF, SVM, log reg. e QDA usados no artigo).
% % usa apenas QBC, entropia e random.
% % }
% \end{itemize}

\section{Cenário realista: Amostragem gnóstica com viés independente}\label{ind}
Numa aplicação real, a disponibilidade de rótulos permite uma escolha mais adequada
do algoritmo de aprendizado.
Assim, estratégias agnósticas e aquelas cujo aprendiz possa ser desvinculado da escolha do
algoritmo de aprendizado definitivo da aplicação se mostram desejáveis.
As estratégias agnósticas adiam naturalmente a escolha do algoritmo para depois da etapa de rotulação.
Entretanto, elas são desprovidas da capacidade de prospecção advinda do víes de aprendizado inerente
a todo aprendiz.
As gnósticas, por sua vez, dependem desse viés desde o início da amostragem ativa.
Apesar disso, é possível adotar um aprendiz compatível com a estratégia e que tenha um bom
desempenho geral; depois de obtidos os rótulos, o algoritmo definitivo seria escolhido via validação
cruzada.

O experimento desta seção foi formulado para avaliar a viabilidade da independência do aprendiz
enquanto alternativa às estratégias agnósticas, simulando uma aplicação real.

A medida de desempenho (ALC-kappa) 

Nas árvores das figuras \ref{treeBestdesf} e  \ref{treedesf},
TUmah, por exemplo, pode ser entendido como uma escolha arriscada
quando o algoritmo é o 5NN ou aceita apenas atributos nominais, ou seja, requer discretização.

\esb{neste exp, a necessidade de aplicação de filtro é separada, ex.: o learner precisa,
mas não obriga o classif a usar ou viceversa;
a mesma ideia foi estendida para a relação estrat-learner, embora pudesse ter sido
feito no experimento anterior (posso aproveitar os resultados daqui lá,
mas faltaria rodar os hits que não coincidem com o bestlearner):
a estratégia precisa (maha), mas não obriga o learner a usar  ou viceversa}

\section{Tempo de espera tolerável}
\ano{para o tempo não é necessário separar por classificador; basta um fried}

\ano{EER venceu demais. no tempo ele vai perder de todas?}

Não é possível auferir tempo durante experimentos de desempenho porque eles são
paralelizados e compartilham cache de processador.

limite de espera pra usuário web é 15s, é o máximo aceitável
\citep{conf/amcis/Nah03}

% Miller (1968) argued for the 2-second rule based on the theory of limitations in human short-
% term memory. According to Miller, short-term memory plays a critical role in human information
% processing; interference with short-term memory can occur when an individual senses an awareness of
% waiting after approximately 2 seconds. Thus, to stay uninterrupted in information processing, the 2-
% second guideline is recommended. For tasks where uninterrupted focus is critical, Nielsen (1995)
% suggests that computer response should be kept within one second.
% 
% 
% From this study, we found that Web users expect a response in about 2 seconds for simple
% information retrieval tasks on the Web.
% A 2-second response is needed to ensure 'smooth' interactions
% between the WWW and the users.  The findings from this
% study also suggest that the upper bound for Web users' TWT is 15 seconds when
% the system does not provide any indication or feedback concerning
% the download

\esb{
é melhor comparar tempos minimos do que tempos medios;
pode haver outlier para cima,
mas para baixo há o limite onde todas as otimizações de hardware já
foram atingidas e a ausência de outros processos é total.
Assim, trata-se de uma grandeza mais comparável entre algoritmos. (ref?)
}

\tar{rodar um classif. rápido de treino e um
rápido de teste em todas as bases, registrar tempo médio de consulta de cada
estratégia e gerar friedman (espera-se realçar vantagem dos agnósticos)}

% 
% e uma faixa de orçamentos:
% \begin{itemize}
%  \item orçamento baixo: $|Y|<\cent\leq min(\frac{|\mathcal{U}|}{2},100)$
%  \item orçamento alto: $min(\frac{|\mathcal{U}|}{2},100)<\cent\leq min(|\mathcal{U}|,200)$
% \end{itemize}

\esb{pra provar que o algoritmo é determinante, basta demonstrar qtas vezes o melhor par contem
o mesmo algoritmo que venceu na passiva}

\ano{faz sentido testar com o melhor classif porque o user vai ter os rotulos pra poder escolher o melhor,
mas isso soh vale pra agnosticas: Rnd, Clu, ATUman, ATUmah e ATUeuc}