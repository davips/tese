\section{Análise comparativa}\label{comparativa}
Há aplicações em que suas particularidades ou preferências do especialista determinam
previamente o algoritmo de aprendizado. \ano{exemplos?}
Por outro lado, há aplicações, possivelmente a maioria, em que a escolha é livre
e concretizada idealmente a partir do momento em que o conjunto de treinamento é
suficientemente grande para a realização de uma validação cruzada \ano{ref?}.
Para simular esses dois cenários de forma concisa e
devido à presença de estratégias específicas de um único tipo de aprendiz
(QBC, EMC, SVMS e SVMB),
a comparação de estratégias está dividida em quatro partes:
\begin{itemize}
 \item \textbf{comparação com algoritmos variados} - compreende algoritmos não vinculados a nenhuma estratégia
 (5NN, C4.5w, CIELM e NB); incidentalmente, esse critério remove os algoritmos que aparecem
 nos extremos da contagem de posições, deixando a contagem de primeiros
e últimos lugares mais equilibrada, entre $21$ e $27$ e entre $19$ e $28$, respectivamente;
 \item \textbf{comparação RFw} - por ser um comitê, o RF viabiliza o uso da estratégia QBC;
  nessa comparação é possível avaliar a existência de benefícios em aplicá-la frente
  às estratégias de uso geral; é possível também observar quais estratégias são adequadas
  para esse algoritmo;
  \item \textbf{comparação SVM} - de forma análoga à \textit{comparação RFw},
  porém com duas estratégias específicas; e,
  \item \textbf{comparação ELM} - de forma análoga à \textit{comparação RFw},
  porém com o diferencial de revelar de forma pioneira a viabilidade de se aplicar
  aprendizado ativo e a estratégia específica com ELMs.
  \end{itemize}

\subsection{Comparação diversa}\label{cdiversa}
% http://research.cs.wisc.edu/techreports/2009/TR1648.pdf
Estratégias são normalmente avaliadas por meio de curvas de aprendizado,
que são os gráficos da medida de interesse em função da quantidade de consultas
\citep{settles2010active}.
% Figure 3 presents learning curves for the first 100 instances labeled using
% uncertainty sampling and random sampling.
% The reported results are for a logistic regression model averaged over ten folds
% using cross-validation.
O comportamento típico da curva de aprendizado ativo seguido por todas as diferentes estratégias
é ilustrado na Figura \ref{curvas}
Apesar da pouca discernibilidade das curvas, a exibição delas para todas as estratégias permite notar
que elas têm a mesma forma logarítmica, com o valor kappa divergindo na medida em que novos
exemplos são consultados.
\input plot
Se todas as bases tivessem a mesma quantidade de exemplos
e o aprendizado prosseguisse, as curvas se encontrariam novamente,
pois tendem ao desempenho passivo.

É possível observar que a curva da DWeuc se destaca negativamente,
mas comparações de curvas neste gráfico são imprecisas devido aos diferentes
pesos que as bases podem ter:
bases mais difíceis tendem a ter um valor mais baixo para kappa e acabam
sub-representadas.
Uma forma de neutralização da desigualdade de nível de dificuldade entre as bases é a adoção
de ranqueamento das médias\footnote{Além das médias,
um gráfico das medianas também foi elaborado e teve um comportamento praticamente igual.
% com leves perturbações irrelevantes
Por motivos de espaço ele foi omitido.}, conforme mostrado na Figura \ref{curvasrank}.
\input rankplot
O fato de nem todas as estratégias agnósticas terem sido favorecidas sugere que a ausência
de premissas baseadas no viés do aprendiz não é condição preponderante para um bom
desempenho em algoritmos de aprendizado variados, como  Rnd e Clu que permaneceram
debaixo da colocação média ($5.5$) por praticamente todo o intervalo de consultas.

Conforme esperado, DWeuc teve o pior desempenho por concentrar as consultas apenas
nas regiões mais densas e de fronteira, deixando de explorar adequadamente o espaço de exemplos.
DWeuc é a estratégia Mar ponderada pela densidade, logo, dada a superioridade de Mar,
a ponderação por densidade pode ser considerada bastante prejudicial quando não leva em
conta os exemplos rotulados.
Por outro lado, as extensões de DW (TU, ATU e GATU) superam as demais, pois ponderam inversamente pela
densidade de exemplos rotulados evitando, assim, as regiões densamente rotuladas.
SGmulti, por sua vez, se manteve próximo à colocação média.
Um evento importante entre as melhores curvas é a inversão entre a estratégia gnóstica
TUmah e sua versão agnóstica ATUmah após aproximadamente $90$ exemplos.
Essa inversão coincide com a intuição de que consultas exploratórias sejam vantajosas no início do
aprendizado, enquanto que consultas prospectivas passem a ser proveitosas mais tardiamente.
Assim, uma estratégia como a GATUmah, capaz de migrar entre agnosticidade e gnosticidade,
pode ter um bom desempenho tanto no início quanto no restante da curva de aprendizado.
A ascensão de Mar também é um indicativo de que a prospecção é prejudicial no início
e vantajosa com o avanço do aprendizado.
A diferença entre EERacu e EERent é um indício de que atacar diretamente a função objetivo
não é tão eficaz quanto o uso da medida de entropia.
Nas análises seguintes, as estratégias DWeuc - por ser claramente superada pelas suas extensões - e
EERacu - por ser melhor representada pela proposta original da EER - foram omitidas.

Para confirmar as conclusões com segurança estatística, os níveis de confiança
($p \in \{0,01; 0,05; 0,10\}$), na Tabela \ref{stratsALCKappaFriedAllReduxall}, indicam
quando uma estratégia na linha é melhor que a outra na coluna.
O critério de vitória é baseado na comparação dos valores da ALC-kappa para todas as bases.
A medida foi calculada com $200$ exemplos nas bases de tamanho suficiente
e $|\mathcal{U}|$ nas demais.
% \input stratsmahALCKappaFriedAllReduxall
\begin{table}[h]
\caption{Um contra um para as estratégias.
Medida: ALC-kappa. \textit{Legenda na Tabela \ref{tab:friedClassif}.}}
\begin{center}\begin{tabular}{lcc|cc|cc|cc}
                        & 1 & 2 & 3 & 4 & 5 & 6 & 7 & 8\\
1 - Rnd         & - &   &   &   &   &   &   &   \\
2 - Clu         &   & - &   &   &   &   &   &   \\ \hline
3 - \textbf{ATUmah}     & * & * & - &   & * &   &   &   \\
4 - \textbf{GATUmah}    & * & * &   & - & * & . &   &   \\ \hline
5 - \textbf{SGmulti}    & * &   &   &   & - &   &   &   \\
6 - Mar         & * & * &   &   & . & - &   &   \\ \hline
7 - TUmah       & * & * &   &   & * &   & - &   \\
8 - EERent      & * & * &   &   & * &   &   & - \\ \hline\end{tabular}
\label{stratsALCKappaFriedAllReduxall}
\end{center}
\end{table}

Como um indicativo da viabilidade do aprendizado ativo em geral, Rnd perde de todas exceto Clu.
Ainda assim, o princípio teórico de Clu, que garante um desempenho que não seja pior que a
amostragem aleatória, manteve-se válido.
Dentre as melhores estratégias, a proposta GATUmah é a preferível, pois além de se equiparar
às outras ela foi superior à Mar - ainda que com baixo nível de confiança ($p=0,10$).
SGmulti superou apenas a aleatória.

% Se forem consideradas apenas as primeiras 100 consultas -
% %150 conf/emnlp/SettlesC08
% valor arbitrário frequentemente adotado em outros trabalhos
% \citep{journals/pieee/CrawfordTY13,chermanaprendizado,conf/nips/SettlesCR07,conf/icml/RoyM01} -
% A comparação passa a ser como na Tabela \ref{stratsALCKappaFriedAllReduxHalf}.
% \input stratsmanALCKappaFriedAllReduxall
% \input stratseucALCKappaFriedAllReduxall
% conforme Tabela \ref{stratsALCKappaFriedAllReduxHalf}.
% $50$ exemplos: quantidade máxima antes que o tamanho das bases permita ALCs de tamanhos diferentes
% - conforme Tabela \ref{stratsALCKappaFriedAllRedux50}.
Em aplicações reais, entretanto, a estabilidade do algoritmo pode ser mais importante
do que o desempenho preditivo médio, pois uma grande variabilidade pode colocar o
orçamento sob o risco de ser gasto sem que seja atingido um desempenho adequado e pode levar a custos
de rotulação imprevisíveis.
Assim, a estabilidade da estratégia tem fundamental importância para a análise do risco financeiro
da aplicação. É importante enfatizar que o processo de rotulação ocorre apenas uma vez em uma
dada reserva de exemplos até o término do orçamento, impossibilitando novas tentativas.

De forma análoga às curvas apresentadas para o valor médio de kappa e o ranqueamento
correspondente, dois conjuntos de curvas baseadas no desvio padrão do valor de kappa foram
construídos visando ilustrar o comportamento da variância preditiva a cada instante ao longo
de sucessivos valores de orçamento.
É possível observar, na Figura \ref{curvasdesvio}, que as estratégias ATUmah e GATUmah
seguidas de TUmah detêm os menores valores de desvio padrão durante a maior parte do
processo de rotulação.
\input plotRisco
O mesmo ocorre com os ranqueamentos apresentados na Figura \ref{curvasrankdesvio},
porém com EERent alternando com TUmah.
\input rankplotRisco
A curva de SGmulti ficou abaixo do ranqueamento médio ($4,5$).
Uma propriedade das curvas é a clara tendência de estabilização durante o treinamento,
indicando que em baixos orçamentos a escolha de estratégias mais estáveis é prioritária.
A significância estatística das diferenças pode ser verificada na Tabela
\ref{stratsALCKappaFriedAllRiscoReduxall}.
% \input stratsmahALCKappaFriedAllRiscoReduxall
\begin{table}[h]
\caption{Um contra um para as estratégias. Medida: desvio padrão da ALC-kappa. \textit{Legenda na Tabela \ref{tab:friedClassif}.}}
\begin{center}\begin{tabular}{lcc|cc|cc|cc}
                                 & 1 & 2 & 3 & 4 & 5 & 6 & 7 & 8\\
1 - Rnd                       & - &   &   &   &   &   &   &   \\
2 - Clu                         &   & - &   &   &   &   &   &   \\ \hline
3 - \textbf{ATUmah}     & * & *  & - &   & *  & * &  & *   \\
4 - \textbf{GATUmah}    & * & * &   & - & * & * &   & * \\ \hline
5 - \textbf{SGmulti}    &   &   &   &    & - &   &   &   \\
6 - Mar                         &   &   &   &   &   & - &   &   \\ \hline
7 - TUmah                       & + & * &   &   & + & * & - & . \\
8 - EERent                      &   &   &   &   &   &   &   & - \\ \hline\end{tabular}
% calculado pelo R
\label{stratsALCKappaFriedAllRiscoReduxall}
\end{center}
\end{table}

Finalmente, pode-se concluir que, segundo o conjunto de bases e algoritmos utilizados,
as versões agnóstica e híbrida propostas (ATUmah e GATUmah) são viáveis enquanto alternativas
à TUmah, com pequena vantagem para GATUmah em acurácia quando considerada
a quantidade de estratégias estatisticamente inferiores a elas.
Com relação à minimização de risco financeiro, 
novamente as estratégias propostas também se mostraram viáveis,
vencendo as mesmas estratégias que TUmah, porém com maior significância estatística.

\subsection{Comparação RFw}\ref{crf}

\subsection{Comparação SVM}\ref{csvm}
\ano{fazer análise adicional do SVM só com problemas binários}
\subsection{Comparação ELM}\ref{celm}
\begin{table}[h]
\caption{Um contra um (ELM). Medida: ALCKappa. \textit{Legenda na Tabela \ref{tab:friedClassif}.}}
\begin{center}\begin{tabular}{lcc|cc|cc|c}
                        & 1 & 2 & 3 & 4 & 5 & 6 & 7\\
1 - Rnd         & - &   &   &   &   &   & * \\
2 - Clu         &   & - &   &   &   &   & * \\ \hline
3 - \textbf{ATUmah}     &   &   & - &   &   &   & * \\
4 - \textbf{GATUmah}    &   &   &   & - &   &   & * \\ \hline
5 - \textbf{SGmulti}    & * & * &   &   & - &   & * \\
6 - Mar         & * & * &   &   &   & - & * \\ \hline
7 - EMC         &   &   &   &   &   &   & - \\\end{tabular}
\quad
\begin{tabular}{rrrrrrr}
  \hline
 & V1 & V2 & V3 & V4 & V5 & V6 \\
  \hline
V2 & 1.00 &  &  &  &  &  \\
  V3 & 0.43 & 0.54 &  &  &  &  \\
  V4 & 0.17 & 0.24 & 1.00 &  &  &  \\
  V5 & 0.00 & 0.00 & 0.50 & 0.82 &  &  \\
  V6 & 0.00 & 0.00 & 0.11 & 0.33 & 0.99 &  \\
  V7 & 0.00 & 0.00 & 0.00 & 0.00 & 0.00 & 0.00 \\
   \hline
\end{tabular}

\label{stratsALCKappaFriedELMRedux}
\end{center}
\end{table}










\section{Análise de nichos}\label{nichos}
Conforme visto na Seção \ref{comparativa}, o algoritmo escolhido pode favorecer ou prejudicar
algumas estratégias.
\ano{conferir elm,svm , outras e rf se isso é verdade}

Nesta seção, as estratégias são comparadas
com o objetivo de identificar nichos em que umas possam se sobressair em relação às outras.
\ano{explicar pela arvore quais os nichos de cada estratégia}

orçamento baixo : $|Y|<\cent\leq min(\frac{|\mathcal{U}|}{2},100)$

orçamento alto: $min(\frac{|\mathcal{U}|}{2},100)<\cent\leq min(|\mathcal{U}|,200)$

Na árvore exibida na Figura \ref{tree},
é possível observar o papel central do algoritmo de aprendizado sobre
a estratégia de aprendizado ativo.
Apesar de ilustrativa, a árvore pode não ser segura do ponto de vista de tomada de
decisão devido ao baixo grau de pureza das folhas.

\input arvore

\input arvorebest

Entretanto, algumas ramificações aceitam explicações plausíveis:
\begin{itemize}
 \item É fato que o algoritmo NB produz estimativas de distribuição de probabilidade
excessivamente confiantes que podem ser amenizadas por
\textit{bagging} \citep{conf/icml/RoyM01}.
Estratégias que dependem dessas estimativas podem ser prejudicadas.
Por outro lado,
por ter um modelo para cada classe, SGmulti pode estar funcionando como \textit{bagging}
\ano{na verdade não é bagging, pois na acurácia é usado apenas um}
e com isso tendo um maior desempenho para esse algoritmo.
A maior afinidade de SGmulti com atributos nominais também é um indício de uma
maior proximidade com NB.
\ano{para avaliar a suavidade de cada learner,
posso calcular a entropia média da saída de cada classif para cada
base em todos os exemplos
e fazer uma tabela com a ultima linha sendo a media de todas as bases}
\esb{When Does Active Learning Work?
confirma que atributos não-discretizados favorecem AL,
isso justifica z-score para qq classificador ? (ao menos RF, SVM, log reg. e QDA usados no artigo).
usa apenas QBC, entropia e random.
}

% \item Rnd tem melhor desempenho em bases mais desbalanceadas
% (entropia igual ou abaixo de $0,95$).
\item Nenhuma folha corresponde à amostragem aleatória.
Isso sugere a viabilidade do aprendizado ativo em geral.
\end{itemize}



% Alguns algoritmos de aprendizado são inadequados para determinadas
% estratégias.
% Por exemplo, aquelas que dependem de estimativas de probabilidade
% não podem ter como aprendiz um classificador que seja capaz de realizar
% apenas predições exatas.
% Para essas estratégias, é necessário haver a possibilidade de avaliação do nível de certeza do modelo.
% % Num cenário em que o algoritmo de aprendizado é fixado pela aplicação,
% % independentemente do problema em questão,
% % a estratégia escolhida deve ser aquela capaz de atingir o melhor desempenho
% % mesmo que o aprendiz não seja o mais adequado aos dados.
% Dessa forma, faz-se necessária uma análise com cada aprendiz isoladamente,
% pois ela evita que classificadores inadequados prejudiquem o desempenho geral.
% Esse cenário pode ser interpretado como aquele em que o algoritmo de aprendizado
% é fixado previamente pela natureza da aplicação.
% As tabelas \ref{stratsALCKappaFriedCIELMRedux},
% \ref{stratsALCKappaFried5NNRedux},
% \ref{stratsALCKappaFriedC4.5Redux},
% \ref{stratsALCKappaFriedNBRedux},
% \ref{stratsALCKappaFriedVFDTRedux} e
% \ref{stratsALCKappaFriedSVMRedux}
% contêm o resultado da comparação de todas as estratégias entre si para
% os algoritmos CIELM, 5NN, C4.5, NB, VFDT e SVM respectivamente \ano{RF?}.
% 
% \input stratsALCKappaFriedELMRedux
% 
% \input stratsALCKappaFriedCIELMRedux
% 
% \input stratsALCKappaFried5NNRedux
% 
% \input stratsALCKappaFriedC4.5wRedux
% 
% \input stratsALCKappaFriedNBRedux
% 
% \input stratsALCKappaFriedRFwRedux
% 
% É possível observar nas seis tabelas uma mudança significativa na distribuição das marcas de
% superioridade estatística.
% A vantagem de uma dada estratégia em relação a outra que use o mesmo aprendiz
% pode chegar a se inverter, como é o caso de EERacu e Clu com os algoritmos
% CIELM e 5NN.
% 
% % o 5-nn foi mal com o SGmulti porque o classificador só consegue expandir os verdadeiros labels para os 5 vizinhos mais próximos. Assim, a área de desacordo abrange praticamente todos os exemplos não rotulados.
% 
% \ano{vfdt:ags e aleas}
% 
% \ano{SVM:EERs e mahalas}
% 
% \ano{c4.5:}
% 
% \ano{nb:SG só não vence manhattans e vice-versa}
% 
% \ano{CI:EERacu; gnos vencem mais que ags}
% 
% \ano{5NN:EERent}
% 


Isso sugere a existência de alguma afinidade entre estratégia e aprendiz
\ano{esmiuçar detalhes?} e também a inexistência de uma estratégia
que seja a melhor com todos os algoritmos de aprendizado.

Algumas observações para cada aprendiz \ano{essas observações servem para
que possamos ter insights sobre o experimento, depois é preciso deixar mais sucinto.}:
\begin{itemize}
 \item CIELM - TUman e TUmah vencem todas as demais exceto Mar;
 \item 5NN - todas as estratégias baseadas em densidade, exceto DWeuc
 (e TUmah em menor grau), vencem as demais, exceto EERent;
 \item C4.5 - similar a 5NN, mas com algumas diferenças relevantes:
  \subitem DWeuc perde de todas;
  \subitem SGmulti vence Rnd e Mar além de DWeuc;
  \subitem ALUman vence sua contraparte e demais estratégias baseadas na
  distância de Mahalanobis;
  \item NB - SGmulti supera todas as estratégias,
  exceto as baseadas na distância de Manhattan; as estratégias agnósticas baseadas na distância de Manhattan vencem todas,
 exceto sua versão gnóstica e SGmulti;
 \item VFDT - similar a NB, mas com algumas diferenças relevantes:
  \subitem é a única vez em que Rnd e Clu vencem mais do que uma das estratégias mais fracas
  \subitem SGmulti deixa de vencer Rnd, Clu
  \subitem SGmulti, ATUman e ALUman deixam de vencer as estratégias
  baseadas na distância de Mahalanobis
  \subitem ATUmah e ALUmah passam a vencer as demais,
  exceto Rnd, Clu, SGmulti, ATUman e ALUman
 \item SVM - EERent vence a maioria das estratégias mais fracas;
 é a única vez em que EERacu vence (as estratégias baseadas na distância de Manhattan);
  é a única vez em que estratégias (baseadas na distância de Mahalanobis)
  vencem todas as demais.
\end{itemize}
\ano{explicitar comprovação da efetividade das propostas}






\section{Tempo de espera tolerável}
\ano{para o tempo não é necessário separar por classificador; basta um fried}

\tar{
fazer árvore de decisão que mostre que metacaracteristicas influenciam
e como no tempo de treinamento?
}

Não é possível auferir tempo durante experimentos de desempenho porque eles são
paralelizados e compartilham cache de processador.

limite de espera pra usuário web é 15s, é o máximo aceitável
\citep{conf/amcis/Nah03}

Miller (1968) argued for the 2-second rule based on the theory of limitations in human short-
term memory. According to Miller, short-term memory plays a critical role in human information
processing; interference with short-term memory can occur when an individual senses an awareness of
waiting after approximately 2 seconds. Thus, to stay uninterrupted in information processing, the 2-
second guideline is recommended. For tasks where uninterrupted focus is critical, Nielsen (1995)
suggests that computer response should be kept within one second.


From this study, we found that Web users expect a response in about 2 seconds for simple
information retrieval tasks on the Web.
A 2-second response is needed to ensure 'smooth' interactions
between the WWW and the users.  The findings from this
study also suggest that the upper bound for Web users' TWT is 15 seconds when
the system does not provide any indication or feedback concerning
the download

\esb{
é melhor comparar tempos minimos do que tempos medios;
pode haver outlier para cima,
mas para baixo há o limite onde todas as otimizações de hardware já
foram atingidas e a ausência de outros processos é total.
Assim, trata-se de uma grandeza mais comparável entre algoritmos. (ref?)
}

\section{rascunho}
\ano{comparar o melhor par estrat/ELM com o melhor par estrat/SVM
seria um bom indicativo de viabiidade da ELM com AL}

\ano{o experimento poderia continuar rodando para o grupo seleto de strats e learns
até que a passiva fosse atingida, assim seria possivel avaliar as mais economicas quando
perdas na acurácia é inaceitável. (a forma menos codificante de fazer isso
é fixar um learner no código,
manter apenas os datasets em que a passiva ainda nao foi atingida visualmente
na tabela, rodar
mais 10 queries e repetir o processo até acabarem os datasets)}

\tar{falta um teste para esse cap. ficar ideal: rodar um classif. rápido de treino e um
rápido de teste em todas as bases, registrar tempo médio de consulta de cada
estratégia e gerar friedman (espera-se realçar vantagem dos agnósticos)}

\ano{Efetividade geral}

\blue{friedman um contra um não está dando nada?}

\blue{friedman talvez funcione melhor por nicho (metaclusters)}

\esb{citar: contra random; comparação 1-a-1; variabilidade (risco);
tolerable waiting time}

\tar{ fazer tabelão friedman (40? pares strat/classif: 40x40)}


\ano{AG,LU vs DWTU}

x Qual a economia proporcionada por uma estratégia?

x Quão bem uma estratégia usufrui de um dado orçamento?

\tar{há alguma correlação entre desempenho de AL e desbalanceamento da base?}

\tar{usar nintera na(s) base(s) à parte?
 ou usar em todas, mas sem strats lentas?}

 % podem tornar a comparação imprecisa,
% pois a diferença entre estratégias se torna irrelevante
% quando nenhuma delas é capaz de atingir um desempenho satisfatório.
% Essa constatação conduz a uma reformulação do experimento anterior visando dois cenários:
% \begin{itemize}
%  \item o algoritmo mais adequado é conhecido - por experiência do usuário sobre o domínio
%  e particularidades de cada classificador ou pela análise de problemas similares já resolvidos;
%  \item o algoritmo mais adequado não é conhecido - o problema é novo e nada pode ser inferido
%  de seus dados.
% \end{itemize}
% Para simular o primeiro caso optou-se pela seleção do algoritmo mais adequado
% (com a melhor acurácia passiva) para cada base.
% No segundo caso, o algoritmo com a acurácia passiva mediana foi considerado
% suficientemente representativo dos demais.
% Uma vantagem adicional desses dois novos cenários é a possibilidade de análise unificada,
% sem a separação por aprendiz feita na Seção \ref{geral}.
%
% \afterpage{\clearpage\begin{landscape}
% \input stratsALCKappaFriedbest
% \end{landscape}\clearpage}
%
% \afterpage{\clearpage\begin{landscape}
% \input stratsALCKappaFriedbestRedux
% \end{landscape}\clearpage}
