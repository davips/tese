\section{Análise Comparativa}\label{analise}
Nesta seção, as estratégias selecionadas na Seção \ref{prep} são comparadas
com o objetivo de identificar nichos em que umas possam se sobressair em relação às outras.
Conjuntamente,
é feita a demontração de que a estratégia agnóstica proposta baseada em densidade é viável.
\ano{explicar pela arvore quais os nichos de cada estratégia}

orçamento baixo : $|Y|<\cent\leq min(\frac{|\mathcal{U}|}{2},100)$

orçamento alto: $min(\frac{|\mathcal{U}|}{2},100)<\cent\leq min(|\mathcal{U}|,200)$

Na árvore exibida na Figura \ref{tree},
é possível observar o papel central do algoritmo de aprendizado sobre
a estratégia de aprendizado ativo.
Apesar de ilustrativa, a árvore pode não ser segura do ponto de vista de tomada de
decisão devido ao baixo grau de pureza das folhas.

\input arvore

Entretanto, algumas ramificações aceitam explicações plausíveis:
\begin{itemize}
 \item É fato que o algoritmo NB produz estimativas de distribuição de probabilidade
excessivamente confiantes que podem ser amenizadas por
\textit{bagging} \citep{conf/icml/RoyM01}.
Estratégias que dependem dessas estimativas podem ser prejudicadas.
Por outro lado,
por ter um modelo para cada classe, SGmulti pode estar funcionando como \textit{bagging}
e com isso tendo um maior desempenho para esse classificador.
A maior afinidade de SGmulti com atributos nominais também é um indício de uma
maior proximidade com NB.
\ano{para avaliar a suavidade de cada learner,
posso calcular a entropia média da saída de cada classif para cada
base em todos os exemplos
e fazer uma tabela com a ultima linha sendo a media de todas as bases}
\esb{When Does Active Learning Work?
confirma que atributos não-discretizados favorecem AL,
isso justifica z-score para qq classificador ? (ao menos RF, SVM, log reg. e QDA usados no artigo).
usa apenas QBC, entropia e random.
}

\item Rnd tem melhor desempenho em bases mais desbalanceadas
(entropia igual ou abaixo de $0,95$).
\item Apenas uma folha corresponde à amostragem aleatória.
Isso sugere a viabilidade do aprendizado ativo em geral.
\end{itemize}

% \input stratsALCKappaFriedSVM
% %
% \input stratsBalAccFriedSVM

\ano{seria ideal fazer uma segunda árvore em que apenas o melhor classificador
é usado em cada base}


\esb{plot de vitórias vs budget, variabilidade/estabilidade, tempo, comparação com passivo -
demonstra que AL é viável em geral e demonstra a efetividade das novas
estratégias propostas
(ponderação pela classe tenta atacar o desbalanceamento, preciso ver se
é mesmo nessas bases que ela se sobressai)}

ref para plot:
Active learning literature survey 2009
http://research.cs.wisc.edu/techreports/2009/TR1648.pdf
...
Active learning algorithms are generally evaluated by constructing learning curves,
which plot the evaluation measure of interest (e.g., accuracy) as a function of the
number of new instance queries that are labeled and added to L.
Figure 3 presents learning curves for the first 100 instances labeled using
uncertainty sampling and random sampling.
The reported results are for a logistic regression model averaged over ten folds
using cross-validation.

\ano{comparar o melhor par estrat/ELM com o melhor par estrat/SVM
seria um bom indicativo de viabiidade da ELM com AL}

\esb{
agrupar por distância entre acurácias (vetor de ~700 dimensões) -
inclui tabela 18x18, grupos de estratégias similares e árvore com hierarquia dos grupos
}

\ano{o experimento poderia continuar rodando para o grupo seleto de strats e learns
até que a passiva fosse atingida, assim seria possivel avaliar as mais economicas quando
perdas na acurácia é inaceitável. (a forma menos codificante de fazer isso
é fixar um learner no código,
manter apenas os datasets em que a passiva ainda nao foi atingida visualmente
na tabela, rodar
mais 10 queries e repetir o processo até acabarem os datasets)}

\tar{falta um teste para esse cap. ficar ideal: rodar um classif. rápido de treino e um
rápido de teste em todas as bases, registrar tempo médio de consulta de cada
estratégia e gerar friedman (espera-se realçar vantagem dos agnósticos)}

\ano{Efetividade geral}

\blue{friedman um contra um não está dando nada?}

\blue{friedman talvez funcione melhor por nicho (metaclusters)}

\esb{citar: contra random; comparação 1-a-1; variabilidade (risco);
tolerable waiting time}

\tar{ fazer tabelão friedman (40? pares strat/classif: 40x40)}


\ano{AG,LU vs DWTU}

x Qual a economia proporcionada por uma estratégia?

x Quão bem uma estratégia usufrui de um dado orçamento?

\tar{há alguma correlação entre desempenho de AL e desbalanceamento da base?}

\tar{usar nintera na(s) base(s) à parte?
 ou usar em todas, mas sem strats lentas?}

\tar{plotar}


\section{Afinidades estratégia-domínio-classificador-orçamento}

\section{Risco}
variabilidade

\section{Tempo de espera tolerável}

\tar{
fazer árvore de decisão que mostre que metacaracteristicas influenciam e como no tempo de treinamento?
}

Não é possível auferir tempo durante experimentos de desempenho porque eles são
paralelizados e compartilham cache de processador.

limite de espera pra usuário web é 15s, é o máximo aceitável
\citep{conf/amcis/Nah03}

Miller (1968) argued for the 2-second rule based on the theory of limitations in human short-
term memory. According to Miller, short-term memory plays a critical role in human information
processing; interference with short-term memory can occur when an individual senses an awareness of
waiting after approximately 2 seconds. Thus, to stay uninterrupted in information processing, the 2-
second guideline is recommended. For tasks where uninterrupted focus is critical, Nielsen (1995)
suggests that computer response should be kept within one second.


From this study, we found that Web users expect a response in about 2 seconds for simple
information retrieval tasks on the Web.
A 2-second response is needed to ensure 'smooth' interactions
between the WWW and the users.  The findings from this
study also suggest that the upper bound for Web users' TWT is 15 seconds when
the system does not provide any indication or feedback concerning
the download

\esb{
é melhor comparar tempos minimos do que tempos medios;
pode haver outlier para cima,
mas para baixo há o limite onde todas as otimizações de hardware já
foram atingidas e a ausência de outros processos é total.
Assim, trata-se de uma grandeza mais comparável entre algoritmos. (ref?)
}