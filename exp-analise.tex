\section{Análise Comparativa}\label{analise}
Nesta seção, as estratégias selecionadas na Seção \ref{prep} são comparadas.
Conjuntamente,
é feita a demonstração de que as estratégias propostas SGmulti,
agnóstica baseada em densidade (prefixo ATU) e agnóstica baseada
na utilidade de cada rótulo (prefixo ALU) são competitivas.
A análise é dividida em duas partes:
na Seção \ref{constante} ela é feita para cada aprendiz separadamente;
na Seção \ref{melhor}, ela é feita com a adoção do melhor
aprendiz para cada base;
e, na Seção \ref{mediano}, ela é feita com a adoção do aprendiz
com desempenho mediano para cada base.

\subsection{Aprendiz constante}\label{constante}
Num cenário em que o algoritmo de aprendizado é fixado pela aplicação,
independentemente do problema em questão,
a estratégia escolhida deve ser aquela capaz de atingir o melhor desempenho
mesmo que o aprendiz não seja o mais adequado aos dados.
Dessa forma, faz-se necessária a análise com cada aprendiz isoladamente.
As tabelas \ref{stratsALCKappaFriedCIELMRedux},
\ref{stratsALCKappaFried5NNRedux},
\ref{stratsALCKappaFriedC4.5Redux},
\ref{stratsALCKappaFriedNBRedux}, 
\ref{stratsALCKappaFriedVFDTRedux} e
\ref{stratsALCKappaFriedSVMRedux}
contém o resultado da comparação de todas as estratégias entre si para
os algoritmos CIELM, 5NN, C4.5, NB, VFDT e SVM respectivamente.

\input stratsALCKappaFriedCIELMRedux

\input stratsALCKappaFried5NNRedux

\input stratsALCKappaFriedC4.5Redux

\input stratsALCKappaFriedNBRedux

\input stratsALCKappaFriedVFDTRedux

\input stratsALCKappaFriedSVMRedux

Quando comparadas entre si,
é possível observar nas tabelas uma mudança significativa na distribuição das marcas de
superioridade estatística.
\ano{interpretar detalhes das tabelas?}
Isso sugere a existência de alguma afinidade entre estratégia e aprendiz 
\ano{esmiuçar detalhes?} e também a inexistência de uma estratégia
que seja a melhor com todos os algoritmos de aprendizado.

Algumas observações para cada aprendiz:
\begin{itemize}
 \item CIELM - TUman e TUmah vencem todas as demais exceto Mar;
 \item 5NN - todas as estratégias baseadas em densidade, exceto DWeuc
 (e TUmah em menor grau), vencem as demais, exceto EERent;
 \item C4.5 - similar a C4.5, mas com algumas diferenças relevantes:
 \begin{itemize}
  \item DWeuc perde de todas;
  \item SGmulti vence também Rnd e Mar;
  \item ALUman vence sua contraparte e demais estratégias baseadas na
  distância de Mahalanobis;
 \end{itemize}
  \item NB - SGmulti supera todas as estratégias,
  exceto as baseadas na distância de Manhattan; as estratégias agnósticas baseadas na distância de Manhattan vencem todas,
 exceto sua versão gnóstica e SGmulti;
 \item VFDT - similar a NB, mas com algumas diferenças relevantes:
 \begin{itemize}
  \item é a única vez em que Rnd e Clu vencem mais do que uma das estratégias mais fracas
  \item SGmulti deixa de vencer Rnd, Clu
  \item SGmulti, ATUman e ALUman deixam de vencer as estratégias
  baseadas na distância de Mahalanobis
  \item ATUmah e ALUmah passam a vencer as demais,
  exceto Rnd, Clu, SGmulti, ATUman e ALUman
 \end{itemize}
 \item SVM - EERent vence a maioria das estratégias mais fracas;
 é a única vez em que EERacu vence (as estratégias baseadas na distância de Manhattan);
  é a única vez em que estratégias (baseadas na distância de Mahalanobis)
  vencem todas as demais.
\end{itemize}

\ano{sumarizar as melhores em geral, ou quando trocam de posição com alguma outra}

\ano{explicitar comprovação da efetividade das propostas}






\subsection{Aprendiz no melhor caso e caso mediano}
Alguns algoritmos de aprendizado são inadequados para determinadas
bases \ano{ref?} e podem tornar a comparação imprecisa,
pois a diferença entre estratégias se torna irrelevante
quando nenhuma delas é capaz de atingir um desempenho satisfatório.
Essa constatação conduz a uma reformulação do experimento anterior visando dois cenários:
\begin{itemize}
 \item o algoritmo mais adequado é conhecido - por experiência do usuário sobre o domínio
 e particularidades de cada classificador ou pela análise de problemas similares já resolvidos;
 \item o algoritmo mais adequado não é conhecido - o problema é novo e nada pode ser inferido
 de seus dados.
\end{itemize}
Para simular o primeiro caso optou-se pela seleção do algoritmo com
a melhor acurácia passiva para cada base.
No segundo caso, o algoritmo com a acurácia passiva mediana foi considerado
suficientemente representativo dos demais.
Uma vantagem adicional desses dois novos cenários é a possibilidade de análise unificada,
sem a separação por aprendiz feita na Seção \ref{}.
% nas bases como um todo, o efeito esperado é aproximar-se do aleatório, mas
% sem seu incoveniente de escolher o pior algoritmo para algumas bases

% \afterpage{\clearpage\begin{landscape}
% \input stratsALCKappabest
% \end{landscape}\clearpage}

\afterpage{\clearpage\begin{landscape}
\input stratsALCKappaFriedbest
\end{landscape}\clearpage}

\afterpage{\clearpage\begin{landscape}
\input stratsALCKappaFriedbestRedux
\end{landscape}\clearpage}


\ano{seria ideal fazer uma segunda árvore em que apenas o melhor classificador
é usado em cada base}


\esb{plot de vitórias vs budget, variabilidade/estabilidade, tempo, comparação com passivo -
demonstra que AL é viável em geral e demonstra a efetividade das novas
estratégias propostas
(ponderação pela classe tenta atacar o desbalanceamento, preciso ver se
é mesmo nessas bases que ela se sobressai)}

ref para plot:
Active learning literature survey 2009
http://research.cs.wisc.edu/techreports/2009/TR1648.pdf
...
Active learning algoritmos are generally evaluated by constructing learning curves,
which plot the evaluation measure of interest (e.g., accuracy) as a function of the
number of new instance queries that are labeled and added to L.
Figure 3 presents learning curves for the first 100 instances labeled using
uncertainty sampling and random sampling.
The reported results are for a logistic regression model averaged over ten folds
using cross-validation.

\section{Risco}
variabilidade
\ano{para o risco não é necessário separar por classificador; basta um fried}

\section{Tempo de espera tolerável}
\ano{para o tempo não é necessário separar por classificador; basta um fried}

\tar{
fazer árvore de decisão que mostre que metacaracteristicas influenciam
e como no tempo de treinamento?
}

Não é possível auferir tempo durante experimentos de desempenho porque eles são
paralelizados e compartilham cache de processador.

limite de espera pra usuário web é 15s, é o máximo aceitável
\citep{conf/amcis/Nah03}

Miller (1968) argued for the 2-second rule based on the theory of limitations in human short-
term memory. According to Miller, short-term memory plays a critical role in human information
processing; interference with short-term memory can occur when an individual senses an awareness of
waiting after approximately 2 seconds. Thus, to stay uninterrupted in information processing, the 2-
second guideline is recommended. For tasks where uninterrupted focus is critical, Nielsen (1995)
suggests that computer response should be kept within one second.


From this study, we found that Web users expect a response in about 2 seconds for simple
information retrieval tasks on the Web.
A 2-second response is needed to ensure 'smooth' interactions
between the WWW and the users.  The findings from this
study also suggest that the upper bound for Web users' TWT is 15 seconds when
the system does not provide any indication or feedback concerning
the download

\esb{
é melhor comparar tempos minimos do que tempos medios;
pode haver outlier para cima,
mas para baixo há o limite onde todas as otimizações de hardware já
foram atingidas e a ausência de outros processos é total.
Assim, trata-se de uma grandeza mais comparável entre algoritmos. (ref?)
}

\section{rascunho}
\ano{comparar o melhor par estrat/ELM com o melhor par estrat/SVM
seria um bom indicativo de viabiidade da ELM com AL}

\ano{o experimento poderia continuar rodando para o grupo seleto de strats e learns
até que a passiva fosse atingida, assim seria possivel avaliar as mais economicas quando
perdas na acurácia é inaceitável. (a forma menos codificante de fazer isso
é fixar um learner no código,
manter apenas os datasets em que a passiva ainda nao foi atingida visualmente
na tabela, rodar
mais 10 queries e repetir o processo até acabarem os datasets)}

\tar{falta um teste para esse cap. ficar ideal: rodar um classif. rápido de treino e um
rápido de teste em todas as bases, registrar tempo médio de consulta de cada
estratégia e gerar friedman (espera-se realçar vantagem dos agnósticos)}

\ano{Efetividade geral}

\blue{friedman um contra um não está dando nada?}

\blue{friedman talvez funcione melhor por nicho (metaclusters)}

\esb{citar: contra random; comparação 1-a-1; variabilidade (risco);
tolerable waiting time}

\tar{ fazer tabelão friedman (40? pares strat/classif: 40x40)}


\ano{AG,LU vs DWTU}

x Qual a economia proporcionada por uma estratégia?

x Quão bem uma estratégia usufrui de um dado orçamento?

\tar{há alguma correlação entre desempenho de AL e desbalanceamento da base?}

\tar{usar nintera na(s) base(s) à parte?
 ou usar em todas, mas sem strats lentas?}
