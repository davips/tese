\section{Análise Comparativa}\label{analise}
Nesta seção, as estratégias selecionadas na Seção \ref{prep} são comparadas.
Conjuntamente,
é feita a demontração de que as estratégias propostas SGmulti,
agnóstica baseada em densidade e agnóstica baseada
na utilidade de cada rótulo são viáveis.

As tabelas \ref{stratsALCKappaFriedSVMRedux}, \ref{stratsALCKappaFried5NNRedux},
\ref{stratsALCKappaFriedC4.5Redux}, \ref{stratsALCKappaFriedCIELMRedux},
\ref{stratsALCKappaFriedVFDTRedux} e \ref{stratsALCKappaFriedNBRedux}
contém o resultado da comparação de todas as estratégias entre si para
os algoritmos SVM, 5NN, C4.5, CIELM, VFDT e NB respectivamente.

\input stratsALCKappaFriedSVMRedux

\input stratsALCKappaFried5NNRedux

\input stratsALCKappaFriedC4.5Redux

\input stratsALCKappaFriedCIELMRedux

\input stratsALCKappaFriedVFDTRedux

\input stratsALCKappaFriedNBRedux

Quando comparadas entre si,
é possível observar nas tabelas uma mudança significativa na distribuição das marcas de
superioridade estatística.
\ano{interpretar detalhes das tabelas?}
Isso sugere a existência de alguma afinidade entre estratégia e aprendiz.
Entretanto, alguns algoritmos de aprendizado são inadequados para determinadas
bases \ano{ref?} e podem tornar a comparação imprecisa,
pois a diferença entre estratégias se torna irrelevante
quando nenhuma delas é capaz de atingir um desempenho satisfatório.
Essa constatação conduz a uma reformulação do experimento visando dois cenários:
\begin{itemize}
 \item o algoritmo mais adequado é conhecido - por conhecimento especializado sobre o domínio
 e particularidades de cada classificador ou pela análise de problemas similares já resolvidos;
 \item o algoritmo mais adequado não é conhecido - o problema é novo e nada pode ser inferido
 de seus dados.
\end{itemize}
Para simular o primeiro caso optou-se pela seleção do algoritmo com
a melhor acurácia passiva para cada base.
No segundo caso, o algoritmo com a acurácia passiva mediana foi considerado
suficientemente representativo dos demais.
% nas bases como um todo, o efeito esperado é aproximar-se do aleatório, mas
% sem seu incoveniente de escolher o pior algoritmo para algumas bases

% \afterpage{\clearpage\begin{landscape}
% \input stratsALCKappabest
% \end{landscape}\clearpage}

\afterpage{\clearpage\begin{landscape}
\input stratsALCKappaFriedbest
\end{landscape}\clearpage}

\afterpage{\clearpage\begin{landscape}
\input stratsALCKappaFriedbestRedux
\end{landscape}\clearpage}


\ano{seria ideal fazer uma segunda árvore em que apenas o melhor classificador
é usado em cada base}


\esb{plot de vitórias vs budget, variabilidade/estabilidade, tempo, comparação com passivo -
demonstra que AL é viável em geral e demonstra a efetividade das novas
estratégias propostas
(ponderação pela classe tenta atacar o desbalanceamento, preciso ver se
é mesmo nessas bases que ela se sobressai)}

ref para plot:
Active learning literature survey 2009
http://research.cs.wisc.edu/techreports/2009/TR1648.pdf
...
Active learning algoritmos are generally evaluated by constructing learning curves,
which plot the evaluation measure of interest (e.g., accuracy) as a function of the
number of new instance queries that are labeled and added to L.
Figure 3 presents learning curves for the first 100 instances labeled using
uncertainty sampling and random sampling.
The reported results are for a logistic regression model averaged over ten folds
using cross-validation.

\ano{comparar o melhor par estrat/ELM com o melhor par estrat/SVM
seria um bom indicativo de viabiidade da ELM com AL}

\esb{
agrupar por distância entre acurácias (vetor de ~700 dimensões) -
inclui tabela 18x18, grupos de estratégias similares e árvore com hierarquia dos grupos
}

\ano{o experimento poderia continuar rodando para o grupo seleto de strats e learns
até que a passiva fosse atingida, assim seria possivel avaliar as mais economicas quando
perdas na acurácia é inaceitável. (a forma menos codificante de fazer isso
é fixar um learner no código,
manter apenas os datasets em que a passiva ainda nao foi atingida visualmente
na tabela, rodar
mais 10 queries e repetir o processo até acabarem os datasets)}

\tar{falta um teste para esse cap. ficar ideal: rodar um classif. rápido de treino e um
rápido de teste em todas as bases, registrar tempo médio de consulta de cada
estratégia e gerar friedman (espera-se realçar vantagem dos agnósticos)}

\ano{Efetividade geral}

\blue{friedman um contra um não está dando nada?}

\blue{friedman talvez funcione melhor por nicho (metaclusters)}

\esb{citar: contra random; comparação 1-a-1; variabilidade (risco);
tolerable waiting time}

\tar{ fazer tabelão friedman (40? pares strat/classif: 40x40)}


\ano{AG,LU vs DWTU}

x Qual a economia proporcionada por uma estratégia?

x Quão bem uma estratégia usufrui de um dado orçamento?

\tar{há alguma correlação entre desempenho de AL e desbalanceamento da base?}

\tar{usar nintera na(s) base(s) à parte?
 ou usar em todas, mas sem strats lentas?}

\tar{plotar}


% \section{Afinidades estratégia-domínio-classificador-orçamento}

\section{Risco}
variabilidade
\ano{para o risco não é necessário separar por classificador; basta um fried}

\section{Tempo de espera tolerável}
\ano{para o tempo não é necessário separar por classificador; basta um fried}

\tar{
fazer árvore de decisão que mostre que metacaracteristicas influenciam
e como no tempo de treinamento?
}

Não é possível auferir tempo durante experimentos de desempenho porque eles são
paralelizados e compartilham cache de processador.

limite de espera pra usuário web é 15s, é o máximo aceitável
\citep{conf/amcis/Nah03}

Miller (1968) argued for the 2-second rule based on the theory of limitations in human short-
term memory. According to Miller, short-term memory plays a critical role in human information
processing; interference with short-term memory can occur when an individual senses an awareness of
waiting after approximately 2 seconds. Thus, to stay uninterrupted in information processing, the 2-
second guideline is recommended. For tasks where uninterrupted focus is critical, Nielsen (1995)
suggests that computer response should be kept within one second.


From this study, we found that Web users expect a response in about 2 seconds for simple
information retrieval tasks on the Web.
A 2-second response is needed to ensure 'smooth' interactions
between the WWW and the users.  The findings from this
study also suggest that the upper bound for Web users' TWT is 15 seconds when
the system does not provide any indication or feedback concerning
the download

\esb{
é melhor comparar tempos minimos do que tempos medios;
pode haver outlier para cima,
mas para baixo há o limite onde todas as otimizações de hardware já
foram atingidas e a ausência de outros processos é total.
Assim, trata-se de uma grandeza mais comparável entre algoritmos. (ref?)
}