\section{Análise comparativa geral}\label{geral}
Há aplicações em que suas particularidades ou preferências do especialista determinam
previamente o algoritmo de aprendizado. \ano{exemplos?}
Por outro lado, há aplicações, possivelmente a maioria, em que a escolha é livre
e concretizada idealmente a partir do momento em que o conjunto de treinamento é
suficientemente grande para a realização de uma validação cruzada \ano{ref?}.
Para simular esses dois cenários de forma concisa e
devido à presença de estratégias específicas de um único tipo de aprendiz
(QBC, EMC, SVMS e SVMB),
a comparação de estratégias está dividida em quatro partes:
\begin{itemize}
 \item \textbf{comparação com algoritmos variados} - compreende algoritmos não vinculados a nenhuma estratégia
 (5NN, C4.5w, CIELM e NB); incidentalmente, esse critério remove os algoritmos que aparecem
 nos extremos da contagem de posições, deixando a contagem de primeiros
e últimos lugares mais equilibrada, entre $21$ e $27$ e entre $19$ e $28$, respectivamente;
 \item \textbf{comparação RFw} - por ser um comitê, o RF viabiliza o uso da estratégia QBC;
  nessa comparação é possível avaliar a existência de benefícios em aplicá-la frente
  às estratégias de uso geral; é possível também observar quais estratégias são adequadas
  para esse algoritmo;
  \item \textbf{comparação SVM} - de forma análoga à \textit{comparação RFw},
  porém com duas estratégias específicas; e,
  \item \textbf{comparação ELM} - de forma análoga à \textit{comparação RFw},
  porém com o diferencial de revelar de forma pioneira a viabilidade de se aplicar
  aprendizado ativo e a estratégia específica com ELMs.
  \end{itemize}

\subsection{Comparação diversa}\label{cdiversa}
% http://research.cs.wisc.edu/techreports/2009/TR1648.pdf
Estratégias são normalmente avaliadas por meio de curvas de aprendizado,
que são os gráficos da medida de interesse em função da quantidade de consultas
\citep{settles2010active}.
% Figure 3 presents learning curves for the first 100 instances labeled using
% uncertainty sampling and random sampling.
% The reported results are for a logistic regression model averaged over ten folds
% using cross-validation.
O comportamento típico da curva de aprendizado ativo seguido por todas as diferentes estratégias
é ilustrado na Figura \ref{curvas}
Apesar da pouca discernibilidade das curvas, a exibição delas para todas as estratégias permite notar
que elas têm a mesma forma logarítmica, com o valor kappa divergindo na medida em que novos
exemplos são consultados.
\input plot
Se todas as bases tivessem a mesma quantidade de exemplos
e o aprendizado prosseguisse, as curvas se encontrariam novamente,
pois tendem ao desempenho passivo.

É possível observar que a curva da DWeuc se destaca negativamente,
mas comparações de curvas neste gráfico são imprecisas devido aos diferentes
pesos que as bases podem ter:
bases mais difíceis tendem a ter um valor mais baixo para kappa e acabam
sub-representadas.
Uma forma de neutralização da desigualdade de nível de dificuldade entre as bases é a adoção
de ranqueamento das médias\footnote{Além das médias,
um gráfico das medianas também foi elaborado e teve um comportamento praticamente igual.
% com leves perturbações irrelevantes
Por motivos de espaço ele foi omitido.}, conforme mostrado na Figura \ref{curvasrank}.
\input rankplot
O fato de nem todas as estratégias agnósticas terem sido favorecidas sugere que a ausência
de premissas baseadas no viés do aprendiz não é condição preponderante para um bom
desempenho em algoritmos de aprendizado variados, como  Rnd e Clu que permaneceram
debaixo da colocação média ($5.5$) por praticamente todo o intervalo de consultas.

Conforme esperado, DWeuc teve o pior desempenho por concentrar as consultas apenas
nas regiões mais densas e de fronteira, deixando de explorar adequadamente o espaço de exemplos.
DWeuc é a estratégia Mar ponderada pela densidade, logo, dada a superioridade de Mar,
a ponderação por densidade pode ser considerada bastante prejudicial quando não leva em
conta os exemplos rotulados.
Por outro lado, as extensões de DW (TU, ATU e GATU) superam as demais, pois ponderam inversamente pela
densidade de exemplos rotulados evitando, assim, as regiões densamente rotuladas.
SGmulti, por sua vez, se manteve próximo à colocação média.
Um evento importante entre as melhores curvas é a inversão entre a estratégia gnóstica
TUmah e sua versão agnóstica ATUmah após aproximadamente $90$ exemplos.
Essa inversão coincide com a intuição de que consultas exploratórias sejam vantajosas no início do
aprendizado, enquanto que consultas prospectivas passem a ser proveitosas mais tardiamente.
Assim, uma estratégia como a GATUmah, capaz de migrar entre agnosticidade e gnosticidade,
pode ter um bom desempenho tanto no início quanto no restante da curva de aprendizado.
A ascensão de Mar também é um indicativo de que a prospecção é prejudicial no início
e vantajosa com o avanço do aprendizado.
A diferença entre EERacu e EERent é um indício de que atacar diretamente a função objetivo
não é tão eficaz quanto o uso da medida de entropia.
Nas análises seguintes, as estratégias DWeuc - por ser claramente superada pelas suas extensões - e
EERacu - por ser melhor representada pela proposta original da EER - foram omitidas.

Para confirmar as conclusões com segurança estatística, os níveis de confiança
($p \in \{0,01; 0,05; 0,10\}$), na Tabela \ref{stratsALCKappaFriedAllReduxall}, indicam
quando uma estratégia na linha é melhor que a outra na coluna.
O critério de vitória é baseado na comparação dos valores da ALC-kappa para todas as bases.
A medida foi calculada com $200$ exemplos nas bases de tamanho suficiente
e $|\mathcal{U}|$ nas demais.
\input stratsmahALCKappaFriedAllReduxall
Como um indicativo da viabilidade do aprendizado ativo em geral, Rnd perde de todas exceto Clu.
Ainda assim, o princípio teórico de Clu, que garante um desempenho que não seja pior que a
amostragem aleatória, manteve-se válido.
Dentre as melhores estratégias, a proposta GATUmah é a preferível, pois além de se equiparar
às outras ela foi superior à Mar - ainda que com baixo nível de confiança ($p=0,10$).
SGmulti superou apenas a aleatória.

% Se forem consideradas apenas as primeiras 100 consultas -
% %150 conf/emnlp/SettlesC08
% valor arbitrário frequentemente adotado em outros trabalhos
% \citep{journals/pieee/CrawfordTY13,chermanaprendizado,conf/nips/SettlesCR07,conf/icml/RoyM01} -
% A comparação passa a ser como na Tabela \ref{stratsALCKappaFriedAllReduxHalf}.
% \input stratsmanALCKappaFriedAllReduxall
% \input stratseucALCKappaFriedAllReduxall
% conforme Tabela \ref{stratsALCKappaFriedAllReduxHalf}.
% $50$ exemplos: quantidade máxima antes que o tamanho das bases permita ALCs de tamanhos diferentes
% - conforme Tabela \ref{stratsALCKappaFriedAllRedux50}.
Em aplicações reais, entretanto, a estabilidade do algoritmo pode ser mais importante
do que o desempenho preditivo médio, pois uma grande variabilidade pode colocar o
orçamento sob o risco de ser gasto sem que seja atingido um desempenho adequado e pode levar a custos
de rotulação imprevisíveis.
Assim, a estabilidade da estratégia tem fundamental importância para a análise do risco financeiro
da aplicação. É importante enfatizar que o processo de rotulação ocorre apenas uma vez em uma
dada reserva de exemplos até o término do orçamento, impossibilitando novas tentativas.

De forma análoga às curvas apresentadas para o valor médio de kappa e o ranqueamento
correspondente, dois conjuntos de curvas baseadas no desvio padrão do valor de kappa foram
construídos visando ilustrar o comportamento da variância preditiva a cada instante ao longo de sucessivos
valores de orçamento.
Na Figura \ref{curvasdesvio}, é possível observar que as estratégias ATUmah e GATUmah
seguidas de TUmah detêm os menores valores de desvio padrão durante a maior parte do
processo de rotulação.
\input plotRisco
O mesmo ocorre com os ranqueamentos apresentados na Figura \ref{curvasrankdesvio},
porém com EERent alternando com TUmah.
\input rankplotRisco
SGmulti teve um desempenho abaixo do ranqueamento médio ($4,5$).

parece que Fried está dizendo não haver diferença, p=1

Finalmente, pode-se concluir, segundo o conjunto de bases e algoritmos utilizados,
que a versão agnóstica proposta (ATUmah) é viável na prática pelo seu desempenho
e por possuir a maior estabilidade.
Ambas as condições são importantes na redução de riscos financeiros.
Adicionalmente, a estratégia híbrida (GATUmah) mostrou-se como a melhor opção
em desempenho, apesar da pequena diferença em relação às outras duas melhores
(ATUmah e TUmah); e 

podendo superar as outras da atual comparação sem incorrer em
aumento do risco financeiro.

\subsection{Comparação RFw}\ref{crf}
\subsection{Comparação SVM}\ref{csvm}
\ano{fazer análise adicional do SVM só com problemas binários}
\subsection{Comparação ELM}\ref{celm}












\esb{comparar também tempo? comparação com passivo?}

\section{Análise comparativa por aprendiz}\label{isolado}
Alguns algoritmos de aprendizado são inadequados para determinadas
estratégias.
Por exemplo, aquelas que dependem de estimativas de probabilidade
não podem ter como aprendiz um classificador que seja capaz de realizar
apenas predições exatas.
Para essas estratégias, é necessário haver a possibilidade de avaliação do nível de certeza do modelo.
% Num cenário em que o algoritmo de aprendizado é fixado pela aplicação,
% independentemente do problema em questão,
% a estratégia escolhida deve ser aquela capaz de atingir o melhor desempenho
% mesmo que o aprendiz não seja o mais adequado aos dados.
Dessa forma, faz-se necessária uma análise com cada aprendiz isoladamente,
pois ela evita que classificadores inadequados prejudiquem o desempenho geral.
Esse cenário pode ser interpretado como aquele em que o algoritmo de aprendizado
é fixado previamente pela natureza da aplicação.
As tabelas \ref{stratsALCKappaFriedCIELMRedux},
\ref{stratsALCKappaFried5NNRedux},
\ref{stratsALCKappaFriedC4.5Redux},
\ref{stratsALCKappaFriedNBRedux},
\ref{stratsALCKappaFriedVFDTRedux} e
\ref{stratsALCKappaFriedSVMRedux}
contêm o resultado da comparação de todas as estratégias entre si para
os algoritmos CIELM, 5NN, C4.5, NB, VFDT e SVM respectivamente \ano{RF?}.

\input stratsALCKappaFriedELMRedux

\input stratsALCKappaFriedCIELMRedux

\input stratsALCKappaFried5NNRedux

\input stratsALCKappaFriedC4.5wRedux

\input stratsALCKappaFriedNBRedux

\input stratsALCKappaFriedRFwRedux

É possível observar nas seis tabelas uma mudança significativa na distribuição das marcas de
superioridade estatística.
A vantagem de uma dada estratégia em relação a outra que use o mesmo aprendiz
pode chegar a se inverter, como é o caso de EERacu e Clu com os algoritmos
CIELM e 5NN.

% o 5-nn foi mal com o SGmulti porque o classificador só consegue expandir os verdadeiros labels para os 5 vizinhos mais próximos. Assim, a área de desacordo abrange praticamente todos os exemplos não rotulados.

\ano{vfdt:ags e aleas}

\ano{SVM:EERs e mahalas}

\ano{c4.5:}

\ano{nb:SG só não vence manhattans e vice-versa}

\ano{CI:EERacu; gnos vencem mais que ags}

\ano{5NN:EERent}



Isso sugere a existência de alguma afinidade entre estratégia e aprendiz
\ano{esmiuçar detalhes?} e também a inexistência de uma estratégia
que seja a melhor com todos os algoritmos de aprendizado.

Algumas observações para cada aprendiz \ano{essas observações servem para
que possamos ter insights sobre o experimento, depois é preciso deixar mais sucinto.}:
\begin{itemize}
 \item CIELM - TUman e TUmah vencem todas as demais exceto Mar;
 \item 5NN - todas as estratégias baseadas em densidade, exceto DWeuc
 (e TUmah em menor grau), vencem as demais, exceto EERent;
 \item C4.5 - similar a 5NN, mas com algumas diferenças relevantes:
  \subitem DWeuc perde de todas;
  \subitem SGmulti vence Rnd e Mar além de DWeuc;
  \subitem ALUman vence sua contraparte e demais estratégias baseadas na
  distância de Mahalanobis;
  \item NB - SGmulti supera todas as estratégias,
  exceto as baseadas na distância de Manhattan; as estratégias agnósticas baseadas na distância de Manhattan vencem todas,
 exceto sua versão gnóstica e SGmulti;
 \item VFDT - similar a NB, mas com algumas diferenças relevantes:
  \subitem é a única vez em que Rnd e Clu vencem mais do que uma das estratégias mais fracas
  \subitem SGmulti deixa de vencer Rnd, Clu
  \subitem SGmulti, ATUman e ALUman deixam de vencer as estratégias
  baseadas na distância de Mahalanobis
  \subitem ATUmah e ALUmah passam a vencer as demais,
  exceto Rnd, Clu, SGmulti, ATUman e ALUman
 \item SVM - EERent vence a maioria das estratégias mais fracas;
 é a única vez em que EERacu vence (as estratégias baseadas na distância de Manhattan);
  é a única vez em que estratégias (baseadas na distância de Mahalanobis)
  vencem todas as demais.
\end{itemize}
\ano{explicitar comprovação da efetividade das propostas}






\section{Tempo de espera tolerável}
\ano{para o tempo não é necessário separar por classificador; basta um fried}

\tar{
fazer árvore de decisão que mostre que metacaracteristicas influenciam
e como no tempo de treinamento?
}

Não é possível auferir tempo durante experimentos de desempenho porque eles são
paralelizados e compartilham cache de processador.

limite de espera pra usuário web é 15s, é o máximo aceitável
\citep{conf/amcis/Nah03}

Miller (1968) argued for the 2-second rule based on the theory of limitations in human short-
term memory. According to Miller, short-term memory plays a critical role in human information
processing; interference with short-term memory can occur when an individual senses an awareness of
waiting after approximately 2 seconds. Thus, to stay uninterrupted in information processing, the 2-
second guideline is recommended. For tasks where uninterrupted focus is critical, Nielsen (1995)
suggests that computer response should be kept within one second.


From this study, we found that Web users expect a response in about 2 seconds for simple
information retrieval tasks on the Web.
A 2-second response is needed to ensure 'smooth' interactions
between the WWW and the users.  The findings from this
study also suggest that the upper bound for Web users' TWT is 15 seconds when
the system does not provide any indication or feedback concerning
the download

\esb{
é melhor comparar tempos minimos do que tempos medios;
pode haver outlier para cima,
mas para baixo há o limite onde todas as otimizações de hardware já
foram atingidas e a ausência de outros processos é total.
Assim, trata-se de uma grandeza mais comparável entre algoritmos. (ref?)
}

\section{rascunho}
\ano{comparar o melhor par estrat/ELM com o melhor par estrat/SVM
seria um bom indicativo de viabiidade da ELM com AL}

\ano{o experimento poderia continuar rodando para o grupo seleto de strats e learns
até que a passiva fosse atingida, assim seria possivel avaliar as mais economicas quando
perdas na acurácia é inaceitável. (a forma menos codificante de fazer isso
é fixar um learner no código,
manter apenas os datasets em que a passiva ainda nao foi atingida visualmente
na tabela, rodar
mais 10 queries e repetir o processo até acabarem os datasets)}

\tar{falta um teste para esse cap. ficar ideal: rodar um classif. rápido de treino e um
rápido de teste em todas as bases, registrar tempo médio de consulta de cada
estratégia e gerar friedman (espera-se realçar vantagem dos agnósticos)}

\ano{Efetividade geral}

\blue{friedman um contra um não está dando nada?}

\blue{friedman talvez funcione melhor por nicho (metaclusters)}

\esb{citar: contra random; comparação 1-a-1; variabilidade (risco);
tolerable waiting time}

\tar{ fazer tabelão friedman (40? pares strat/classif: 40x40)}


\ano{AG,LU vs DWTU}

x Qual a economia proporcionada por uma estratégia?

x Quão bem uma estratégia usufrui de um dado orçamento?

\tar{há alguma correlação entre desempenho de AL e desbalanceamento da base?}

\tar{usar nintera na(s) base(s) à parte?
 ou usar em todas, mas sem strats lentas?}

 % podem tornar a comparação imprecisa,
% pois a diferença entre estratégias se torna irrelevante
% quando nenhuma delas é capaz de atingir um desempenho satisfatório.
% Essa constatação conduz a uma reformulação do experimento anterior visando dois cenários:
% \begin{itemize}
%  \item o algoritmo mais adequado é conhecido - por experiência do usuário sobre o domínio
%  e particularidades de cada classificador ou pela análise de problemas similares já resolvidos;
%  \item o algoritmo mais adequado não é conhecido - o problema é novo e nada pode ser inferido
%  de seus dados.
% \end{itemize}
% Para simular o primeiro caso optou-se pela seleção do algoritmo mais adequado
% (com a melhor acurácia passiva) para cada base.
% No segundo caso, o algoritmo com a acurácia passiva mediana foi considerado
% suficientemente representativo dos demais.
% Uma vantagem adicional desses dois novos cenários é a possibilidade de análise unificada,
% sem a separação por aprendiz feita na Seção \ref{geral}.
%
% \afterpage{\clearpage\begin{landscape}
% \input stratsALCKappaFriedbest
% \end{landscape}\clearpage}
%
% \afterpage{\clearpage\begin{landscape}
% \input stratsALCKappaFriedbestRedux
% \end{landscape}\clearpage}
