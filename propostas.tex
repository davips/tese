\chapter{Propostas} \label{propostas}
O tema desta tese é o uso efetivo do aprendizado de máquina ativo,
diante da diversidade existente de estratégias.

\esb{esboça a estrutura conjunta deste e dos próximos 4 capítulos;
explica que a contribuição se divide em três partes: análise comparativa,
viabilidade de ap. ativo pra ELM e meta-aprendizado; os detalhes
são dados em seus respectivos capítulos e a metodologia geral
vem antes}

\ano{revisar totalmente conforme nova estrutura de capitulos}

Neste capítulo são apresentadas as contribuições e trabalhos derivados da pesquisa
realizada.
Foi desenvolvida uma biblioteca \citep{doi/ml} de interface com o
\textit{software} Weka
que serviu de base para as implementações mencionadas nas próximas seções.

\tar{mencionar em cada seção o artigo em que foi ou vai ser publicado o tema}

\ano{agDW* levam vantagem talvez porque com classificadores hard (NB, C45?, ...)
a estimativa de informatividade (a estimativa da distribuição de probabilidade)
fica prejudicada pela presença do learner.
É preciso comparar classificador a classifcador.}

% % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % 
\esb{pra onde vai isso?}
A implementação dos algoritmos aqui propostos e de existentes na literatura
culminou em uma biblioteca extensível de código aberto específica para aprendizado ativo.
Ela está disponível publicamente para fins de pesquisa e replicabilidade \citep{doi/al}.


\section{Busca no espaço de hipóteses multiclasse}
Conforme explicado na Seção \ref{sgnet},
é possível fazer uma amostragem ativa dentro
da perspectiva do espaço de hipóteses.
Entretanto, a abordagem original exposta na Seção \ref{sgnet}
contempla apenas problemas de classificação binária.
Para contornar essa limitação, uma adaptação chamada \textit{SGmulti}
foi proposta \citep{conf/hais/SantosC14}.

Diferentemente da abordagem original em que existe uma classe positiva
que define um modelo mais geral e um mais específico,
na proposta de adaptação um modelo específico é induzido para cada classe.
Cada modelo, assim, tem uma aproximação da hipótese mais geral
($\theta_G$) e a conjunção de todos os modelos contém
aproximações das hipóteses mais específicas de cada classe ($\theta_S$).
Isso é feito por meio da rotulação artificial de todos os exemplos presentes
na reserva de acordo com a meta de cada modelo,
que é apresentar a hipótese mais geral para uma dada classe.
Uma vez gerados os modelos com os exemplos artificiais,
os exemplos para consulta passam a ser amostrados da região de desacordo
entre todos os modelos.

A ordem de complexidade do \textit{SG-multi} é $\mathcal{O}(|Y|)$.

\ano{mesclar descrição abaixo que é mais formal com a descrição acima?}

Para cada classe $c \in Y$, existe um par modelo/conjunto de treinamento
$\langle\theta_c,\mathcal{L}_c\rangle$
devidamente projetado para representar a hipótese mais geral
$h^c_G$ com relação à classe $c$.
Inicialmente, todos os exemplos
$\langle\bm{x},y,w\rangle \in \mathcal{L}_c$
são os mesmos exemplos da reserva, exceto por duas diferenças:
eles são rotulados como positivos ``positive'' para a classe correspondente
($y=c$) e lhes são atribuídos pesos com apenas uma pequena fração
do peso dos exemplos que já têm suas classes reveladas
($w << 1$),
como sugerido recentemente na literatura \cite{series/synthesis/2012Settles}.
O valor adotado para o peso neste trabalho foi
$w=\frac{1}{|Y||\mathcal{U}|}$,
pois ele assegura que a soma de toda a influência dos exemplos de fundo
seja menor que o peso de um único exemplo real.
Essa medida evita que os exemplos de fundo se sobreponham aos
exemplos reais no início do processo de rotulação - momento de escassez
de exemplos reais.

A função de predição $f(\theta_c, \bm{x})$ retorna
a classe mais provável de um dado exemplo $\bm{x}$
de acordo com o modelo fornecido $\theta_c$.
É possível encontrar um exemplo controverso $\bm{x}^*$ \ano{sem consenso}
comparando as saídas de todas as diferentes funções de predição.
Cada função de predição representa o conceito mais geral
de cada classe:
\[
  \forall a,b \in Y, a \neq b, \exists \bm{x}^* \mid f(\theta_a,\bm{x}^*) \neq f(\theta_b,\bm{x}^*)
\]

Assim que um exemplo da região de desacordo $\bm{x}^*$ é consultado
ele substitui seu exemplo de fundo correspondente em todos os conjuntos
de treinamento com seu rótulo real e peso integral:
 \[
  \mathcal{L}_c \leftarrow (\mathcal{L}_c - \{\langle\bm{x}^*,c,w\rangle\}) \cup
\{\langle\bm{x}^*,c,1\rangle\} \forall c \in Y
 \]


\section{Adaptações da amostragem ponderada por densidade}

\subsection{Agnóstica}\label{ag}
A estratégia DWTU pondera a incerteza de forma a afastar as consultas
dos exemplos já rotulados e a aproximá-las das maiores concentrações de exemplos
não rotulados. Essa característica faz com que os exemplos tendam a ser consultados
da região de fronteira e arredores, mesmo que todos os exemplos
estivessem debaixo de um mesmo nível de incerteza.
Isso significa que o classificador e sua medida de incerteza representam um papel que
pode ser redundante, de mero ajuste fino ou mesmo atrapalhar em parte dos casos.
Essa possibilidade de redundância aliada à ideia de agnosticismo da amostragem baseada
em agrupamento motivou a adaptação da amostragem ponderada por densidade por meio
da remoção da medida de incerteza da fórmula inicial previamente apresentada nas equações
\ref{eqid} e \ref{eqtu}. A nova fórmula é apresentada na Equação \ref{eq:agtu}.
\begin{equation}\label{eq:agtu}
 TU(\bm{x}) =
 \frac{1}{|\mathcal{U}|} \sum_{\bm{u} \in \mathcal{U}} sim(\bm{x},\bm{u})
 (\sum_{\bm{l} \in \mathcal{L}} sim(\bm{x},\bm{l}))^{-1}
\end{equation}
Assim, possivelmente sem grande impacto na acurácia,
pode-se adiar a escolha do algoritmo de aprendizado para depois da obtenção dos rótulos.
Em determinadas aplicações também pode ser útil a possibilidade de se conhecer toda
a sequência de consultas antes de começar a interação com o oráculo.
Frequentemente, o número de consultas é conhecido de antemão \citep{settles2010active};
isso permite, por exemplo, que elas sejam repartidas entre vários oráculos que podem
trabalhar de forma independente dos demais.
Essa possibilidade não existe na estratégia de agrupamento hierárquico,
pois ela faz uso dos rótulos para determinar a pureza de cada grupo.
Outra vantagem é a inexistência de tempo de espera interconsultas.

\ano{Desvantagem do cluster-based
        é pesado, mesmo que o aprendiz seja leve isso não é aproveitado, pois é agnóstico
}

\subsection{Ponderada pela utilidade de cada classe}\label{lu}
% ELM vai bem com essa

Uma possibilidade de extensão da amostragem baseada em densidade é
ampliar os tipos de grupos de interesse do par original
\textit{rotulados}/\textit{não rotulados}
para \textit{um grupo por classe}/\textit{não rotulados}:

\ano{colocar fórmula da DWLU2}

A motivação para tal mudança reside no potencial equilíbrio
que as densidades por classe possam trazer em bases desbalanceadas.
Grupos de exemplos da classe majoritária repelem as consultas mais
fortemente do que grupos da minoritária, possivelmente aumentando
a quantidade de consultas a exemplos da classe minoritária.

\subsection{Agnóstica ponderada pela utilidade de cada classe}\label{aglu}
% vai bem com vários learners
Naturalmente, as propostas das seções \ref{ag} e \ref{lu} podem ser combinadas.

\ano{colocar fórmula; vai ser a da AgLU2}

\input exp-elm

\section{SemiELM}
\ano{investigar maneiras de incorporar unlabeled data na ELM}
 
\section{Considerações}
