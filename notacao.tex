\section{Terminologia e notação}\label{notacao}
O conjunto de símbolos e termos empregados neste texto são definidos nesta seção.
Ele é baseado no livro de \cite{series/synthesis/2012Settles}, mas adaptado para ser compatível
com a literatura de \elms e meta-aprendizado
\citep{journals/tsmc/HuangZDZ12,books/daglib/0022052}.
\ano{realmente usa alguma notação de meta-aprendizado?}
Embora a maioria dos algoritmos empregados nos experimentos sejam por
\ing{lote de exemplos}{batch}, a notação assume que o aprendizado ocorre incrementalmente
por meio do subscrito $(t), t \in \mathbb{N}$ que representa a iteração ou quantidade de exemplos
incorporados ao treinamento até o momento.
O uso de parênteses visa diferenciá-lo mais claramente do subscrito convencional de vetores que
indica o valor para uma dada posição.

O termo \textit{classificador} é utilizado quando se trata do produto esperado do processo
realizado pelo \textit{algoritmo de aprendizado}.
A classificação de exemplos se baseia no último modelo $\theta$ induzido pelo algoritmo.
Durante a indução, se o algoritmo integrar uma estratégia de aprendizado ativo,
ele é chamado de \textit{aprendiz}.
Apesar de contradizerem o princípio motivador do aprendizado ativo explicitado
na Seção \ref{aprendizado-ativo},
existem estratégias sem aprendiz, chamadas neste trabalho de \textit{agnósticas}.
\cite{conf/isaim/DasguptaHM08} define o termo de outro ponto de vista:
``um \textit{aprendiz ativo agnóstico} é aquele que não assume a existência de uma
fronteira de decisão perfeita'', porém a definição sobre a ausência de aprendiz
é mais conveniente nesta tese.
% an agnostic active learner (one that does not assume a perfect separator exists)
% http://machinelearning.wustl.edu/mlpapers/paper_files/NIPS2007_178.pdf
% o cenário de aprendizado agnóstico não faz suposições a respeito da origem dos rótulos.
% This is often referred to as the agnostic learning setting,
% since we make no assumptions about the origin of the labels.
% http://www.jennwv.com/courses/F11/lecture4.pdf

Os exemplos ainda não consultados são chamados, neste trabalho,
de \textit{candidatos}. \ano{onde?}
Por simplicidade, o aprendizado é dito \textit{passivo} se for realizado com todos os exemplos e
todos os rótulos presentes.
A estratégia de amostragem de maior correspondência com o aprendizado passivo é a aleatória,
pois tende a ser uma aproximação não enviesada de todo o conjunto de exemplos.

Cada \textit{consulta} de uma dada estratégia ao \textit{oráculo} visa a obtenção
do \textit{rótulo} que indica a \textit{classe} de um dado exemplo.
A determinação da classe de um exemplo é chamada de \textit{rotulação}.
Uma classe $c$ pertence ao conjunto de classes possíveis $C$ para um dado problema e
é representada por um vetor binário $\bm{y} \in Y$,
sendo $Y$ formalmente definido pela Equação \ref{ydef}.
\begin{equation}\label{ydef}
 Y = \{\bm{y} \mid y_o=1, y_p=0 \forall o \neq p, 1 \leq p \leq |C|\}
\end{equation}
Essa representação binária, por exemplo, $\Y=\{(0,0,1),(0,1,0),(1,0,0)\}$,
é conhecida como \ing{um-de-n}{one-hot encoding} (\cite{Harris:2007:DDC}).
O vetor $\bm{y}=\{0,1,0\}$ é um exemplo de \textit{representação de classe} e é
chamado apenas de \textit{classe} de um dado exemplo por conveniência.

O indicador de iteração $(t)$ é usado quando o contexto exige situar o aprendizado
no tempo, na medida em que a quantidade de exemplos aumenta um a um.
O conjunto de valores assumidos pelos atributos \textit{descritivos} de um exemplo
é dado por uma tupla\footnote {Ou vetor, quando o contexto for de álgebra linear.}
$\bm{x}$ pertencente ao conjunto $X$ de tuplas possíveis de atributos descritivos
(exemplos não rotulados).
Dessa forma, a representação completa de um exemplo é um par
%  \ano{dupla/tupla?}
$\langle \bm{x}_{(t)}, \bm{y}_{(t)} \rangle \in X\times Y$.
Os valores possíveis de cada componente de $\bm{x}$ são dados por um conjunto
$\bm{a}$ pertencente a $A$. $A$ é o conjunto de atributos descritivos de um dado problema.
Assim, cada atributo $\bm{a} \in A$ é o conjunto de todos os valores que cada componente
$b \in \{x_i \mid 1 \leq i < \dime{\bm{x}}\}$ da tupla/vetor $\bm{x}$ pode assumir,
por exemplo $\bm{a}=\{\textit{alto},\textit{medio},\textit{baixo}\}$ e $b=medio$ ou
$\bm{a} = \mathbb{R}$ e $b=3.7$.
O tipo de atributo é dado pela função $\nom:A\rightarrow\{0,1\}$ que retorna $1$ para
atributos nominais e $0$ para atributos numéricos.

Dado um exemplo $\bm{x}$ e supondo que o modelo $\theta$ permita
estimação da probabilidade de $\bm{x}$ ter a classe $\bm{y}$,
ela é denotada por $P_{\theta}$,
sendo que $\hat{\bm{y}}$ é classe mais mais provável conforme a Equação \ref{p}.
\begin{equation}\label{p}
  \hat{\bm{y}} = \argmax_{\bm{y}} P_{\theta} (\bm{y}|\bm{x})
\end{equation}
$P_{\theta}$ é a base de grande parte das medidas de informatividade $Inf(\bm{x})$ de exemplos.
% Adicionalmente, e considerando uma construção incremental da hipótese
% (de forma a incorporar uma componente temporal $t$ de instante/ordem de chegada),
% o algoritmo de treinamento do modelo $\theta$ pode ser representado por
% $a: \langle \{\theta_1, \theta_2, ...\},X,Y \rangle \rightarrow \{\theta_1, \theta_2, ...\}$, de forma que cada novo modelo $\theta^{(t+1)}$ é dado por:
% \begin{equation}
%   \theta^{(t+1)} = a(\theta^{(t)}, \langle \bm{x},y \rangle), \forall t \in \mathbb{N^{+}}
% \end{equation}
A classe \textit{minoritária} é a de menor ocorrência e a \textit{majoritária} a de maior ocorrência.
Os símbolos $\mathcal{U}$ e $\mathcal{L}$ representam a
reserva de exemplos \citep{series/synthesis/2012Settles}
e o conjunto de exemplos já rotulados, respectivamente.
$\cent$ é o valor do orçamento ou, em outras palavras, o número de consultas permitido.



%   \begin{equation}
% h: \mathcal{D} \rightarrow \{-,+\}
% \end{equation}

% Assim, o domínio de uma dado problema de classificação é definido pela tupla
% $\{X,Y,A\}$
As consultas podem ser \ing{exploratórias}{for exploration}, que buscam maximizar
a variedade na escolha de exemplos; ou \ing{prospectivas}{for exploitation},
que se concentram apenas nos casos mais críticos,
ou seja, com maior \textit{informatividade}.
O limite de duração para processamento computacional entre consultas
é chamado \ing{tempo de espera tolerável}{tolerable waiting time},
pois é a espera efetivamente sentida pelo oráculo.
