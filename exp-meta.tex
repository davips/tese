\chapter{Aprendizado Meta-ativo}\label{aml}
\tar{rodei meta multirrotulo pra vários metaclassif:
c45=0.50 Maj=0.46}

\tar{vou rodar meta monorrotulo pra vários metaclassif com trios lea-bud-dataset sem
roubar:
c45=0.19 Maj=0.14}

\ano{resume livro de meta-ap.; cita Marcilio, statlog;
falar do desafio de nao usar informacoes sobre as classes nas metafeatures
que existe em metaAL e não em metaPassiveL}

\ano{como a predição da melhor não vence a majoritaria preciso usar ranking
(ainda posso testar algumas variações, como fixar
cada um dos learners possíveis)}


\ano{Acho que NFL se refere à impossibilidade de aprendizado sem viés
no caso de ap. ativo trata-se mais de amostragem do que de aprendizado;
as estrategias agnósticas são um exemplo de pura amostragem;
para o AL ser enxergado como um problema de aprendizado é
preciso enxergá-lo como um problema de classificar cada exemplo como
``consultar'' ou ``não consultar'';}

\ano{isso acima poderia até se tornar um ensemble de AL que trabalha em cima de exemplos
transformados pelas diferentes estratégias
- um classificador único que aprenderia de vários datasets
quais exemplos são bons de consultar,
mas infelizmente é totalmente inviável implementar isso agora;
ele seria treinado com a sequencia otima de exemplos-transformados de
cada base: os primeiros seriam os positivos e últimos seriam os negativos.
)}

\tar{meta-aprendizado pra recomendar quando não usar pode ter maior acurácia.}

\ano{diferenciar do meta-aprendizado ativo do Prudêncio}

Multiarmed bandit problem
http://www.ualberta.ca/~szepesva/Thesis/varun08.thesis.pdf

\section{Recomendação de estratégias}

\ano{recomendação de estratégia para base,
budget e aprendiz dados [e qtd inicial de rótulos?]}

\ano{escolhe uma dentre várias}

\ano{escolhe cada uma contra random}

\ano{mostra quando random é a melhor (interessante)}

\ano{mostra quando random é a melhor ou empata}

\ano{recomendação de aprendiz
(NB,C45,etc. e ''não-use essa estrat.'' ou ''use aleatório'')
para uma dada estratégia}

\tar{metaap pra decidir entre AG, LU ou TU}

\tar{carac. dos classifs como metafeature:
\begin{itemize}
 \item sharp ou soft (alternativa: entropia média das predições)
\end{itemize}
}

\ano{sucesso garantido: recomendar strat por tempo de consulta estimado.
seria muito óbvio?}

\section{Meta-estratégia}
\ano{\citep{conf/ijcnn/SoutoPSACLS08}, sobre clustering,
usam ranking médio como default:
Method SRC
Default 0.59 +- 0.37
Meta-Leaner 0.75 +- 0.21
;
``melhoram`` statlog, aplicando log}

\citep{kalousis2002algorithm} pg. 43 tem metafeatures (inclusive histograma,
mas como as faixas ideais são imprevisíveis, resolvi isso colocando max, min, avg e min/max)
Posso colocar ou omitir a informação de distribuição das classes; ela normalmente não
é conhecida na prática.

\ano{vai que o tempo de phd se multiplica:
testar com outros meta-classificadores; Meta-estratégia dinâmica}

\esb{A quantidade inicial de exemplos rotulados tem sido fixada igual ao número Y de classes.
Uma meta-feature N interessante seria adicionar uma etapa inicial exploratória
que sorteia N exemplos para todas as estratégias
(os valores poderiam ser 0 e 12, pois há um pool com apenas 50 exemplos,
12 é metade do máximo aceitável pra AL que é 0.5 do total).
Evita a influência das diferenças ruidosas no início das curvas de aprendizado.
Ajuda DW e também ajuda ClusterBased, cuja implementação não aproveita os rótulos iniciais (e pode dispensar a etapa inicial).
Porém, qto maios exemplos, mais as estratégias se aproximam de Random Sampling.}

tese kalousis: The first attempt to characterize datasets in order to predict the performance of classication algorithms was done by Rendell et al. (1987).

tese bruno feres:


\section{Experimentos e Resultados}
\ano{citar programa R para a extração de parte dos meta-atributos \citep{team2010r}}

\section{Considerações}
liga com próximo capítulo apontando para as propostas de estratégias e de metaap.;
resumir o motivo da proposta