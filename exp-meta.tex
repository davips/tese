\chapter{Aprendizado Meta-ativo}\label{aml}
\esb{O mais próximo de meta que encontrei na literatura foi a combinação MAB de estratégias:
http://arxiv.org/pdf/1309.6830.pdf}

\tar{parece claro pelos exps anteriores que algumas estratégias simplesmente não aceitam
certos aprendizes (ainda falta explicitar isso lá, apontando caso a caso e justificando
teoricamente, para evitar que seja dito que ocorreu overfitting quando essa info
for usada no metaap),
então isso facilitaria um metaap que indique pares strat/aprendiz,pois
reduziria a quantidade de combinações possíveis}

\ano{ainda é preciso ver se ranking supera ranking medio (default)}

\ano{possibilidades: um experimento para cada learner separadamente;
sortear budget no exp acima;
sortear learners num experimento só;}


\section{Análise de nichos}\label{nichos}

Nesta seção, as estratégias são comparadas
com o objetivo de identificar nichos em que umas possam se sobressair em relação às outras.
\ano{explicar pela arvore quais os nichos de cada estratégia}

orçamento baixo : $|Y|<\cent\leq min(\frac{|\mathcal{U}|}{2},100)$

orçamento alto: $min(\frac{|\mathcal{U}|}{2},100)<\cent\leq min(|\mathcal{U}|,200)$

Na árvore exibida na Figura \ref{tree},
é possível observar o papel central do algoritmo de aprendizado sobre
a estratégia de aprendizado ativo.
Apesar de ilustrativa, a árvore pode não ser segura do ponto de vista de tomada de
decisão devido ao baixo grau de pureza das folhas.

\input arvore

\input arvorebest

Entretanto, algumas ramificações aceitam explicações plausíveis:
\begin{itemize}
 \item É fato que o algoritmo NB produz estimativas de distribuição de probabilidade
excessivamente confiantes que podem ser amenizadas por
\textit{bagging} \citep{conf/icml/RoyM01}.
Estratégias que dependem dessas estimativas podem ser prejudicadas.
Por outro lado,
por ter um modelo para cada classe, SGmulti pode estar funcionando como \textit{bagging}
e com isso tendo um maior desempenho para esse classificador.
A maior afinidade de SGmulti com atributos nominais também é um indício de uma
maior proximidade com NB.
\ano{para avaliar a suavidade de cada learner,
posso calcular a entropia média da saída de cada classif para cada
base em todos os exemplos
e fazer uma tabela com a ultima linha sendo a media de todas as bases}
\esb{When Does Active Learning Work?
confirma que atributos não-discretizados favorecem AL,
isso justifica z-score para qq classificador ? (ao menos RF, SVM, log reg. e QDA usados no artigo).
usa apenas QBC, entropia e random.
}

% \item Rnd tem melhor desempenho em bases mais desbalanceadas
% (entropia igual ou abaixo de $0,95$).
\item Nenhuma folha corresponde à amostragem aleatória.
Isso sugere a viabilidade do aprendizado ativo em geral.
\end{itemize}



\tar{LOO é usado; então para ter confiança est. pode-se adotar o teste de McNemar}

\tar{rodei meta multirrotulo pra vários metaclassif:
c45=0.50 Maj=0.46}

\tar{vou rodar meta monorrotulo pra vários metaclassif com trios lea-bud-dataset sem
roubar:
c45=0.19 Maj=0.14}

\ano{resume livro de meta-ap.; cita Marcilio, statlog;
falar do desafio de nao usar informacoes sobre as classes nas metafeatures
que existe em metaAL e não em metaPassiveL}

\ano{como a predição da melhor não vence a majoritaria preciso usar ranking
(ainda posso testar algumas variações, como fixar
cada um dos learners possíveis)}


\ano{Acho que NFL se refere à impossibilidade de aprendizado sem viés
no caso de ap. ativo trata-se mais de amostragem do que de aprendizado;
as estrategias agnósticas são um exemplo de pura amostragem;
para o AL ser enxergado como um problema de aprendizado é
preciso enxergá-lo como um problema de classificar cada exemplo como
``consultar'' ou ``não consultar'';}

\ano{isso acima poderia até se tornar um ensemble de AL que trabalha em cima de exemplos
transformados pelas diferentes estratégias
- um classificador único que aprenderia de vários datasets
quais exemplos são bons de consultar,
mas infelizmente é totalmente inviável implementar isso agora;
ele seria treinado com a sequencia otima de exemplos-transformados de
cada base: os primeiros seriam os positivos e últimos seriam os negativos.
)}

\tar{meta-aprendizado pra recomendar quando não usar pode ter maior acurácia.}

\ano{diferenciar do meta-aprendizado ativo do Prudêncio}

Multiarmed bandit problem
http://www.ualberta.ca/~szepesva/Thesis/varun08.thesis.pdf

\section{Recomendação de estratégias}
\esb{devido à escassez de metaexemplos, LOO foi adotado em conjunto com treinamento
em grupos de exemplos}

\ano{recomendação de estratégia para base,
budget e aprendiz dados [e qtd inicial de rótulos?]}

\ano{escolhe uma dentre várias}

\ano{escolhe cada uma contra random}

\ano{mostra quando random é a melhor (interessante)}

\ano{mostra quando random é a melhor ou empata}

\ano{recomendação de aprendiz
(NB,C45,etc. e ''não-use essa estrat.'' ou ''use aleatório'')
para uma dada estratégia}

\tar{metaap pra decidir entre AG, LU ou TU}

\tar{carac. dos classifs como metafeature:
\begin{itemize}
 \item sharp ou soft (alternativa: entropia média das predições)
\end{itemize}
}

\ano{sucesso garantido: recomendar strat por tempo de consulta estimado.
seria muito óbvio?}

\section{Meta-estratégia}
\tar{mais metafeatures:
 só usa atributos nominais (NB,...)
2- só usa atributos numéricos (SVM, ELM, ...)
3- sofre com atributos irrelevantes (5NN ou medir impacto de adicionar atts irrelevantes)
4- seleciona atributos internamente (árvores)
5- fornece boas estimativas de probabilidade (medir entropia média pra saber)
6- é baseado em distância (5NN)
7- fronteira de decisao horiz e vertical (arvore)}

\ano{\citep{conf/ijcnn/SoutoPSACLS08}, sobre clustering,
usam ranking médio como default:
Method SRC
Default 0.59 +- 0.37
Meta-Leaner 0.75 +- 0.21
;
``melhoram`` statlog, aplicando log}

\citep{kalousis2002algorithm} pg. 43 tem metafeatures (inclusive histograma,
mas como as faixas ideais são imprevisíveis, resolvi isso colocando max, min, avg e min/max)
Posso colocar ou omitir a informação de distribuição das classes; ela normalmente não
é conhecida na prática.

\ano{vai que o tempo de phd se multiplica:
testar com outros meta-classificadores; Meta-estratégia dinâmica}

\esb{A quantidade inicial de exemplos rotulados tem sido fixada igual ao número Y de classes.
Uma meta-feature N interessante seria adicionar uma etapa inicial exploratória
que sorteia N exemplos para todas as estratégias
(os valores poderiam ser 0 e 12, pois há um pool com apenas 50 exemplos,
12 é metade do máximo aceitável pra AL que é 0.5 do total).
Evita a influência das diferenças ruidosas no início das curvas de aprendizado.
Ajuda DW e também ajuda ClusterBased, cuja implementação não aproveita os rótulos iniciais (e pode dispensar a etapa inicial).
Porém, qto maios exemplos, mais as estratégias se aproximam de Random Sampling.}

tese kalousis: The first attempt to characterize datasets in order to predict the performance of classication algorithms was done by Rendell et al. (1987).

tese bruno feres:


\section{Experimentos e Resultados}
\ano{citar programa R para a extração de parte dos meta-atributos \citep{team2010r}}

\section{Considerações}
liga com próximo capítulo apontando para as propostas de estratégias e de metaap.;
resumir o motivo da proposta


